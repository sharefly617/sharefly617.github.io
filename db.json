{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"themes/miccall/source/css/backcss.css","path":"css/backcss.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/demo.css","path":"css/demo.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/gallery.min.css","path":"css/gallery.min.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/font-awesome.min.css","path":"css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/gallery.css","path":"css/gallery.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/dropdownMenu.css","path":"css/dropdownMenu.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/item.css","path":"css/item.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/mic_gallery.css","path":"css/mic_gallery.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/mic_main.css","path":"css/mic_main.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/my_link.css","path":"css/my_link.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/normalize.css","path":"css/normalize.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/noscript.css","path":"css/noscript.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/ok_prism.css","path":"css/ok_prism.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism.css","path":"css/prism.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_coy.css","path":"css/prism_coy.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_dark.css","path":"css/prism_dark.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_default.css","path":"css/prism_default.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_funky.css","path":"css/prism_funky.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_okaidia.css","path":"css/prism_okaidia.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_solarized.css","path":"css/prism_solarized.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/prism_twilight.css","path":"css/prism_twilight.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/typo.css","path":"css/typo.css","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/FZLT.ttf","path":"fonts/FZLT.ttf","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/FontAwesome.otf","path":"fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.eot","path":"fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.woff","path":"fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.ttf","path":"fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.svg","path":"fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.woff2","path":"fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/gallery_skel.min.js","path":"js/gallery_skel.min.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/jquery.min.js","path":"js/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/jquery.scrollex.min.js","path":"js/jquery.scrollex.min.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/lazyload.min.js","path":"js/lazyload.min.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/jquery.scrolly.min.js","path":"js/jquery.scrolly.min.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/mic_gallery.js","path":"js/mic_gallery.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/prism.js","path":"js/prism.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/skel.min.js","path":"js/skel.min.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/util.js","path":"js/util.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/12.jpg","path":"images/12.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/21.jpg","path":"images/21.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/3123.jpg","path":"images/3123.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/me.jpg","path":"images/me.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/overlay.png","path":"images/overlay.png","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic03.jpg","path":"images/pic03.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic02.jpg","path":"images/pic02.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic04.jpg","path":"images/pic04.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic05.jpg","path":"images/pic05.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic01.jpg","path":"images/pic01.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic06.jpg","path":"images/pic06.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic08.jpg","path":"images/pic08.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic07.jpg","path":"images/pic07.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic09.jpg","path":"images/pic09.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic11.jpg","path":"images/pic11.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic12.jpg","path":"images/pic12.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/picx03.jpg","path":"images/picx03.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/pic10.jpg","path":"images/pic10.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/呗.jpg","path":"images/呗.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/img/ba.jpg","path":"img/ba.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/img/logo_miccall.png","path":"img/logo_miccall.png","modified":0,"renderable":1},{"_id":"themes/miccall/source/img/bg.jpg","path":"img/bg.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/arrow.svg","path":"css/images/arrow.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/arrow-small.svg","path":"css/images/arrow-small.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/close-small.svg","path":"css/images/close-small.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/close.svg","path":"css/images/close.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/close-small-alt.svg","path":"css/images/close-small-alt.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/spinner.svg","path":"css/images/spinner.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/open.svg","path":"css/images/open.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/css/images/open-small.svg","path":"css/images/open-small.svg","modified":0,"renderable":1},{"_id":"themes/miccall/source/js/gallery/gallery.js","path":"js/gallery/gallery.js","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/01.jpg","path":"images/fulls/01.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/02.jpg","path":"images/fulls/02.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/03.jpg","path":"images/fulls/03.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/04.jpg","path":"images/fulls/04.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/05.jpg","path":"images/fulls/05.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/06.jpg","path":"images/fulls/06.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/08.jpg","path":"images/fulls/08.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/07.jpg","path":"images/fulls/07.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/10.jpg","path":"images/fulls/10.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/09.jpg","path":"images/fulls/09.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/11.jpg","path":"images/fulls/11.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/fulls/12.jpg","path":"images/fulls/12.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/01.jpg","path":"images/thumbs/01.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/02.jpg","path":"images/thumbs/02.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/04.jpg","path":"images/thumbs/04.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/03.jpg","path":"images/thumbs/03.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/06.jpg","path":"images/thumbs/06.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/05.jpg","path":"images/thumbs/05.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/09.jpg","path":"images/thumbs/09.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/07.jpg","path":"images/thumbs/07.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/08.jpg","path":"images/thumbs/08.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/10.jpg","path":"images/thumbs/10.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/11.jpg","path":"images/thumbs/11.jpg","modified":0,"renderable":1},{"_id":"themes/miccall/source/images/thumbs/12.jpg","path":"images/thumbs/12.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/MCMC系列：Metropololis-Hasting采样算法，10分钟从零到大神.md","hash":"1707c0c35fb03d2bbb5b390a5c821fe7eee61cb9","modified":1654144457007},{"_id":"source/_posts/Tensor 储存字典的key，如何将字典的value填入Tensor.md","hash":"71c45dd772ccc6d01b2fbc016ded743ec6a2395f","modified":1654144568491},{"_id":"source/_posts/Trace—σ代数的证明.md","hash":"576486eed26ccbff875bb0d6994fc493bdf9661e","modified":1654144491540},{"_id":"source/_posts/hello-world.md","hash":"acad91ace80b80295b11a9b7ad4c29a2dcfdd8fb","modified":1654081725809},{"_id":"source/_posts/复随机变量.md","hash":"1cc68fd9d4563f830c91d3230b0eed2ed76767df","modified":1654144417072},{"_id":"source/_posts/官方文档没讲清楚？？？Pytorch中神奇函数方法map()_.md","hash":"8e8c0ffa9eb092e48dff34286776d2a1ec55fa9f","modified":1654144637314},{"_id":"source/_posts/正态分布推导瑞利分布，瑞利信道的模型.md","hash":"37b6744efa20ae3a434cca4136e80567c220b82a","modified":1654093935833},{"_id":"source/_posts/高斯分布的数乘，相加，相乘还是高斯分布吗？.md","hash":"6229ee02d8ccd996c17cb135b1d5f06130e4f291","modified":1654144356891},{"_id":"source/_posts/论文导读：使用松弛半定规划的方法进行数据降维--通过最大最小距离.md","hash":"2b94feb3e2cc2fd53d193627b740d130b7592b26","modified":1654093415776},{"_id":"themes/miccall/LICENSE","hash":"2ea9f02239dc6b5fdbfff01fcdf85bcc8c13667c","modified":1654082688929},{"_id":"themes/miccall/_config.yml","hash":"4cc7088e1c89de79cf544c1cf10bd6e834367a0e","modified":1654092671129},{"_id":"themes/miccall/readme.md","hash":"265b03fc7f82a391c34508857344df9bb1e11625","modified":1654082688936},{"_id":"themes/miccall/layout/index.ejs","hash":"7478c414bc8eac98c335b61649b3d43e1cba5c6f","modified":1654082688936},{"_id":"themes/miccall/layout/layout.ejs","hash":"b7134f32ae661f2bd7bfa6d14a2408b19eb288ef","modified":1654082688936},{"_id":"themes/miccall/layout/post.ejs","hash":"1208744dfdb7f70ed64d3ddad1ef826898e2800a","modified":1654082688936},{"_id":"themes/miccall/layout/_partial/copyright.ejs","hash":"1b900fcff36deef63d48b567b62427cbdfac2aad","modified":1654082688930},{"_id":"themes/miccall/layout/_partial/comment.ejs","hash":"22c8ddb0d9033257664954bd2e2ee996a95e5dc3","modified":1654082688929},{"_id":"themes/miccall/layout/_partial/footer.ejs","hash":"6f81a21679280023b329db2b729ea5fc902d32bf","modified":1654082688930},{"_id":"themes/miccall/layout/_partial/gallery_head.ejs","hash":"6995a50e8648cfabff6d00ed6c3ffc33aeeb13bd","modified":1654082688930},{"_id":"themes/miccall/layout/_partial/gallery_js.ejs","hash":"123c3b0e446ce6f00561195ccf06c3fa2bb9b67c","modified":1654082688930},{"_id":"themes/miccall/layout/_partial/headerlogo.ejs","hash":"16755bfb4be7ad831ca951bb68ecd3cd2fe28106","modified":1654082688931},{"_id":"themes/miccall/layout/_partial/head.ejs","hash":"e224aba715d3f830db435501383559aba813dbb8","modified":1654082688930},{"_id":"themes/miccall/layout/_partial/importJS.ejs","hash":"d19ca227416d6b52aad5ee119b01cec04171e468","modified":1654082688931},{"_id":"themes/miccall/layout/_partial/intro.ejs","hash":"2952def928e752064cf967ed4c3585455b159644","modified":1654082688931},{"_id":"themes/miccall/layout/_partial/mathjax.ejs","hash":"bc58e1e1cd56f53081b96e63ad2b21b60ce04cd4","modified":1654082688931},{"_id":"themes/miccall/layout/_partial/mainfirst.ejs","hash":"1c731e0c0aa6cea88e0185036da9e12b1c2ea2f6","modified":1654082688931},{"_id":"themes/miccall/layout/_partial/nav.ejs","hash":"7aa4415f720084d457924314580b89ff710af424","modified":1654082688932},{"_id":"themes/miccall/layout/_widget/page-gallery.ejs","hash":"13c1414ae48ede101902f589032bfd74487b40c8","modified":1654082688935},{"_id":"themes/miccall/layout/_widget/page-links.ejs","hash":"c31b1ca6389ba2fd297fed8d8c32f2207c8510fa","modified":1654082688935},{"_id":"themes/miccall/layout/_widget/page-tagcloud.ejs","hash":"bc891c99c4f2312ea8946dc401332ab82043f4fb","modified":1654082688935},{"_id":"themes/miccall/source/css/backcss.css","hash":"cf72e1e50fd17bd69c15ed939b71fcede0ef6c14","modified":1654082688937},{"_id":"themes/miccall/source/css/demo.css","hash":"aafc11b3cb9910853ef26acb491e24f2880e3e3e","modified":1654082688937},{"_id":"themes/miccall/source/css/gallery.min.css","hash":"c5333ca835aae49239d809130a7b281af745729e","modified":1654082688938},{"_id":"themes/miccall/source/css/font-awesome.min.css","hash":"a715d31a96ccab32670e709082e40cbf11497d61","modified":1654082688938},{"_id":"themes/miccall/source/css/gallery.css","hash":"69eea5facb3a5991fe74d7fb9460bbedc3ffae86","modified":1654082688938},{"_id":"themes/miccall/source/css/dropdownMenu.css","hash":"6198125bb51428ff612b62bab879409a1b4c73a9","modified":1654082688937},{"_id":"themes/miccall/source/css/item.css","hash":"89ef8ad5c7c22476d4c8b53106072b4e593f5446","modified":1654082688941},{"_id":"themes/miccall/source/css/mic_gallery.css","hash":"6a85430e1b17ad13ad7f2ae45617899b2cb0dd56","modified":1654082688941},{"_id":"themes/miccall/source/css/my_link.css","hash":"3d89cfd70d9d50fcde140d818b5d8e82424c0497","modified":1654082688942},{"_id":"themes/miccall/source/css/normalize.css","hash":"a0ff1e2feee0b1bb7e54a88fd6367ba3e7435aaf","modified":1654082688942},{"_id":"themes/miccall/source/css/noscript.css","hash":"721fafde7779732ee13de81be1c1c5baa3655cd2","modified":1654082688942},{"_id":"themes/miccall/source/css/ok_prism.css","hash":"54c6c07bbfdd4f677443b183166b87475dee2d74","modified":1654082688942},{"_id":"themes/miccall/source/css/prism.css","hash":"4d824f5ffeca752156d1fbf96565be187f880ce2","modified":1654082688943},{"_id":"themes/miccall/source/css/prism_coy.css","hash":"3608bd92724b39bc4e5307bfb3eb682e97be0ad8","modified":1654082688943},{"_id":"themes/miccall/source/css/prism_dark.css","hash":"dd3166d33870e9a21550762c978b95331e0f20a3","modified":1654082688943},{"_id":"themes/miccall/source/css/prism_funky.css","hash":"59db786b898ed8aa067e64ad4e54dd94631b075b","modified":1654082688944},{"_id":"themes/miccall/source/css/prism_default.css","hash":"7d18c8e21f463a7880c4d6a444cbf573494640e6","modified":1654082688943},{"_id":"themes/miccall/source/css/prism_okaidia.css","hash":"54c6c07bbfdd4f677443b183166b87475dee2d74","modified":1654082688944},{"_id":"themes/miccall/source/css/prism_solarized.css","hash":"554f4c2d24b1824211ffbaa3eb74b5b4db3dc34e","modified":1654082688944},{"_id":"themes/miccall/source/css/prism_twilight.css","hash":"3aec78eea4c35812bfac6b776f4fea79d4d7668e","modified":1654082688944},{"_id":"themes/miccall/source/css/typo.css","hash":"843e6cc6b1004219a850781c58bab3d221b00ddb","modified":1654082688945},{"_id":"themes/miccall/source/js/gallery_skel.min.js","hash":"955b722a7638bb20970efbc79e12d8bb628a36a6","modified":1654082688986},{"_id":"themes/miccall/source/js/jquery.scrollex.min.js","hash":"ede14d2a018d6df0ffd13514dc4662724ad8f9c9","modified":1654082688987},{"_id":"themes/miccall/source/js/lazyload.min.js","hash":"bccbdcb5e5c13543dc5cc764073e3a454739d458","modified":1654082688988},{"_id":"themes/miccall/source/js/jquery.scrolly.min.js","hash":"91ee4deda8189fde4432a8f58cfe3b5f2aed9dcf","modified":1654082688987},{"_id":"themes/miccall/source/js/main.js","hash":"8bffd0514ed17f52c76cedb4d4424660467d7879","modified":1654082688988},{"_id":"themes/miccall/source/js/mic_gallery.js","hash":"344ae990dcac628af652badc4574b72bc416020c","modified":1654082688988},{"_id":"themes/miccall/source/js/skel.min.js","hash":"955b722a7638bb20970efbc79e12d8bb628a36a6","modified":1654082688990},{"_id":"themes/miccall/source/js/util.js","hash":"e32e958f74bd5edc4e1fbdd9fa6c30425d3c7954","modified":1654082688990},{"_id":"themes/miccall/source/images/12.jpg","hash":"88d68b169744a76ca7b21a258fc0812bc03ee701","modified":1654082688962},{"_id":"themes/miccall/source/images/3123.jpg","hash":"fc005184ff616231647c5dc85c63167040d810ba","modified":1654082688964},{"_id":"themes/miccall/source/images/overlay.png","hash":"e978d81201508e6fcbe0fe95485dae1a4b983a8d","modified":1654082688975},{"_id":"themes/miccall/source/images/pic03.jpg","hash":"ec46b8b1490a794ea5d4c61058959b9a52ac262f","modified":1654082688975},{"_id":"themes/miccall/source/images/pic02.jpg","hash":"ab77495fef5102f94e36b71d4308cd682dd4b1a8","modified":1654082688975},{"_id":"themes/miccall/source/images/pic04.jpg","hash":"ee299cd40108e50d7151333de525049f8ec4f150","modified":1654082688976},{"_id":"themes/miccall/source/images/pic05.jpg","hash":"b74d638c0aa9e90f94b056db4c0a886e382973f1","modified":1654082688976},{"_id":"themes/miccall/source/images/pic01.jpg","hash":"774db644cdac607e0d9d72ca65a4961f9a250c11","modified":1654082688975},{"_id":"themes/miccall/source/images/pic06.jpg","hash":"3d8572663edf8a77b99bdfd93491d4f6a7298aaf","modified":1654082688976},{"_id":"themes/miccall/source/images/pic08.jpg","hash":"993e8ff91c20b6d28b6302d3a18367547610c58a","modified":1654082688977},{"_id":"themes/miccall/source/images/pic07.jpg","hash":"de06b4eee3d0e42ddcac415adc8c83466f430253","modified":1654082688977},{"_id":"themes/miccall/source/images/pic09.jpg","hash":"6aae76e8215e3a6e785ce2a65484e4df03ed5fe9","modified":1654082688977},{"_id":"themes/miccall/source/images/pic11.jpg","hash":"a034beb80041d238f36685706a019a269f159ac1","modified":1654082688978},{"_id":"themes/miccall/source/images/pic12.jpg","hash":"ae93f2674a3b1f1dcbfe9692f2e172e106c02d96","modified":1654082688978},{"_id":"themes/miccall/source/images/picx03.jpg","hash":"cffaa38c9489f131f6f37763b37b937811b316df","modified":1654082688978},{"_id":"themes/miccall/source/images/pic10.jpg","hash":"814a661f2966a3c8fe2c89034f62e828f291d770","modified":1654082688977},{"_id":"themes/miccall/source/images/呗.jpg","hash":"b520aa4d797fc0794d93104f260d18a523b67522","modified":1654082688974},{"_id":"themes/miccall/source/css/images/arrow.svg","hash":"84dba9fe4dffab02e5e32f66cea1e38c12129a7c","modified":1654082688939},{"_id":"themes/miccall/source/css/images/arrow-small.svg","hash":"ecc2016f117d725f156bdb60a81e4d09d88961e1","modified":1654082688939},{"_id":"themes/miccall/source/css/images/close-small.svg","hash":"92f0c906a7540f0bac4e4aad1387362f218ee02f","modified":1654082688940},{"_id":"themes/miccall/source/css/images/close.svg","hash":"205739dd10d087f013583087e012ee7423a99c34","modified":1654082688940},{"_id":"themes/miccall/source/css/images/close-small-alt.svg","hash":"7908654ee26cd247d52fb4fae6ad6b8060a00598","modified":1654082688939},{"_id":"themes/miccall/source/css/images/spinner.svg","hash":"285d4c263d37d920d65a1c67c870a05dbbdbb7c3","modified":1654082688940},{"_id":"themes/miccall/source/css/images/open.svg","hash":"5e61408876123f8a53a1491899b1c813934c299b","modified":1654082688940},{"_id":"themes/miccall/source/css/images/open-small.svg","hash":"068e6e2c79172fc81d8f1471750fd6f0658c682f","modified":1654082688940},{"_id":"themes/miccall/source/js/gallery/gallery.js","hash":"79688611831faca31d5ef0c6d90397c29781fa98","modified":1654082688986},{"_id":"themes/miccall/source/images/fulls/01.jpg","hash":"cfc928948301b97ab2873e837284cd83cc3c6cff","modified":1654082688965},{"_id":"themes/miccall/source/images/fulls/02.jpg","hash":"1a729f597316a7483ebcf39b5984ced1612a0410","modified":1654082688966},{"_id":"themes/miccall/source/images/fulls/03.jpg","hash":"0f8c8109f8b605aa46ef571df1d16e8bdd43d090","modified":1654082688967},{"_id":"themes/miccall/source/images/fulls/04.jpg","hash":"806679d4c4398335413b6b53941709a4aef7c3cd","modified":1654082688967},{"_id":"themes/miccall/source/images/fulls/05.jpg","hash":"232b0b9e3b6e751ebb2e6c4284651639526200e3","modified":1654082688968},{"_id":"themes/miccall/source/images/fulls/06.jpg","hash":"e97b29127a50c69cf445d0591cd94daab0f50e33","modified":1654082688969},{"_id":"themes/miccall/source/images/fulls/08.jpg","hash":"d31c389218f04bc7ec510ac236fbe343048e3cfc","modified":1654082688970},{"_id":"themes/miccall/source/images/fulls/07.jpg","hash":"eb41675450650b613195806b154c6badbb7eca4d","modified":1654082688969},{"_id":"themes/miccall/source/images/fulls/10.jpg","hash":"9a284568752a54ab9e8e9cd19c41750e05975d2a","modified":1654082688971},{"_id":"themes/miccall/source/images/fulls/09.jpg","hash":"61fab3aa8ceee938efb4965042ce2b5f51d3a584","modified":1654082688971},{"_id":"themes/miccall/source/images/fulls/11.jpg","hash":"cff6219163e3154c1263ee1d7f924c0ae3ccc9e7","modified":1654082688972},{"_id":"themes/miccall/source/images/fulls/12.jpg","hash":"a631491537584e2e23926d9d6f50ce60bb884b18","modified":1654082688973},{"_id":"themes/miccall/source/images/thumbs/01.jpg","hash":"4f5a2b9fdf44146d60f8dd3a32c81a8419b9148f","modified":1654082688979},{"_id":"themes/miccall/source/images/thumbs/02.jpg","hash":"396a422cbc1b94b6c9b00f6a34c02b05913230ba","modified":1654082688979},{"_id":"themes/miccall/source/images/thumbs/04.jpg","hash":"3c60445217b1b52b1fd6cae4578356c564d7624d","modified":1654082688980},{"_id":"themes/miccall/source/images/thumbs/03.jpg","hash":"e3915b1cc3283b5ec33f49e2591ebf2e6be2362e","modified":1654082688979},{"_id":"themes/miccall/source/images/thumbs/06.jpg","hash":"ffc80bb5fe28626d9462e06d9f01d2a5f99b0ae9","modified":1654082688980},{"_id":"themes/miccall/source/images/thumbs/05.jpg","hash":"3e28c2253db93ba42d16ba9a1f451dbc452fd947","modified":1654082688980},{"_id":"themes/miccall/source/images/thumbs/09.jpg","hash":"bc8a02decf65e44657f02dcc11c7395bfa41ffdb","modified":1654082688981},{"_id":"themes/miccall/source/images/thumbs/07.jpg","hash":"45194a98089345e4f135c0f0bfffeee7ccdd9c99","modified":1654082688980},{"_id":"themes/miccall/source/images/thumbs/08.jpg","hash":"e0384120f6acfa64ad0b5bd0d7e114e9f239294e","modified":1654082688981},{"_id":"themes/miccall/source/images/thumbs/10.jpg","hash":"ccfee49fbca187bad971336cb8fda7dec05ab01e","modified":1654082688981},{"_id":"themes/miccall/source/images/thumbs/12.jpg","hash":"17ab685447ed06b6c12e4d42f38111c684221bc7","modified":1654082688982},{"_id":"themes/miccall/layout/_widget/comment/changyan/common.ejs","hash":"49f6580a358c75d152ec98c3e064e202ca188528","modified":1654082688932},{"_id":"themes/miccall/source/images/thumbs/11.jpg","hash":"66a7fb237824b1b2fd550a068bb6a32f5031ab8f","modified":1654082688981},{"_id":"themes/miccall/layout/_widget/comment/changyan/enter.ejs","hash":"f2728b174b12c3dc62b0199f540473ffb2a03ad8","modified":1654082688932},{"_id":"themes/miccall/layout/_widget/comment/changyan/main.ejs","hash":"720b1fd954577382ebdb60de6a93fb875843a9bb","modified":1654082688933},{"_id":"themes/miccall/layout/_widget/comment/valine/enter.ejs","hash":"691bdffce98b55d89977e610400a8a416c3b8750","modified":1654082688934},{"_id":"themes/miccall/layout/_widget/comment/valine/main.ejs","hash":"a2a8441a533bdb60848f9b6da92b9b79fb6ea0de","modified":1654082688935},{"_id":"themes/miccall/layout/_widget/comment/disqus_click/common.ejs","hash":"83605554328fc4456b88452247a4ba26b42bdcad","modified":1654082688934},{"_id":"themes/miccall/layout/_widget/comment/disqus_click/enter.ejs","hash":"2af0b1276184decab109496e359cb81733c3fa27","modified":1654082688934},{"_id":"themes/miccall/layout/_widget/comment/disqus_click/main.ejs","hash":"f4fb0405a22ec75caa75de6cc8804e26a00e52e3","modified":1654082688934},{"_id":"themes/miccall/layout/_widget/comment/disqus/common.ejs","hash":"c3e8ce6eb296b9b23836beaaf08b04d5e4ae5c11","modified":1654082688933},{"_id":"themes/miccall/layout/_widget/comment/disqus/enter.ejs","hash":"69ef53a73be6827772f2b5e8cadf8a13cf6a938c","modified":1654082688933},{"_id":"themes/miccall/layout/_widget/comment/disqus/main.ejs","hash":"371884a4108219877c49563c0a07f0b9bc6363f8","modified":1654082688933},{"_id":"themes/miccall/source/css/mic_main.css","hash":"630edeb7a261f85b8cf77c7b80f333852024fec3","modified":1654082688942},{"_id":"themes/miccall/source/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1654082688957},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1654082688958},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1654082688961},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1654082688961},{"_id":"themes/miccall/source/js/jquery.min.js","hash":"e6082a7b52db82604ac446d2e6a32cb5af263781","modified":1654082688987},{"_id":"themes/miccall/source/images/logo.png","hash":"ee2cf634d838f8acab5b826400edb0b42c8ce52b","modified":1654082688974},{"_id":"themes/miccall/source/img/logo_miccall.png","hash":"ee2cf634d838f8acab5b826400edb0b42c8ce52b","modified":1654082688985},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1654082688960},{"_id":"themes/miccall/source/images/me.jpg","hash":"853d6623c9bbde9ed66855a27782f1e01f7f24d5","modified":1617090937703},{"_id":"themes/miccall/source/js/prism.js","hash":"21582bba61751fe137d814b9f6ecd77e74655330","modified":1654082688990},{"_id":"themes/miccall/source/fonts/fontawesome-webfont.svg","hash":"964e8dac2cee79c4b49b5bc300675773f6518812","modified":1654082688959},{"_id":"themes/miccall/source/images/21.jpg","hash":"0b2570df0e6d9b79a0c50d0b4ce8799bd40ea421","modified":1654082688964},{"_id":"themes/miccall/source/img/ba.jpg","hash":"af7fe6067d0c21f558502613eed86d8246000bba","modified":1654082688985},{"_id":"themes/miccall/source/img/bg.jpg","hash":"76c3942791aef649ebd4776b5f768c9f2a881400","modified":1530104735213},{"_id":"themes/miccall/source/fonts/FZLT.ttf","hash":"24dfab81236612d596c97eb38b3adf5de99f669c","modified":1654082688956},{"_id":"public/2022/06/02/MCMC系列：Metropololis-Hasting采样算法，10分钟从零到大神/index.html","hash":"8eb9d01d147d261973f767df59752508f60cae53","modified":1654144644304},{"_id":"public/2022/06/02/Tensor 储存字典的key，如何将字典的value填入Tensor/index.html","hash":"d45abc3bfed9ccf76faa6a6e8bd34a1510e0c3e7","modified":1654144644304},{"_id":"public/2022/06/02/Trace—σ代数的证明/index.html","hash":"f87275b40a5f876d261920bbe9cc55c5804f559f","modified":1654144644304},{"_id":"public/2022/06/02/官方文档没讲清楚？？？Pytorch中神奇函数方法map()_/index.html","hash":"6dcf6e7aca067443569763d6ccd7ee23ae9de32d","modified":1654144644304},{"_id":"public/2022/06/02/复随机变量/index.html","hash":"3e9639cbab2f3ffafe902991546a244ea742ba9e","modified":1654144644304},{"_id":"public/2022/06/02/高斯分布的数乘，相加，相乘还是高斯分布吗？/index.html","hash":"bd2ebf7e46bf858299ffe048dbb446ea0e874ebd","modified":1654144644304},{"_id":"public/2022/06/01/正态分布推导瑞利分布，瑞利信道的模型/index.html","hash":"11f9dcb22149148a9cd91c1a9fa40e482e41eda2","modified":1654144644304},{"_id":"public/2022/06/01/hello-world/index.html","hash":"4186f2eecf8b20a34634998fd92af43886c30be5","modified":1654144644304},{"_id":"public/categories/编程/index.html","hash":"7a3752d2b7b93feea44e533617de9ab2abbd3976","modified":1654144644304},{"_id":"public/tags/统计学/index.html","hash":"1cadf0581e0b2b6822f65796130a2cea515dd5fe","modified":1654144644304},{"_id":"public/tags/tensorflow/index.html","hash":"f246eb0d83b7418a59a73b3b47ef6daca3a67e76","modified":1654144644304},{"_id":"public/tags/Pytorch/index.html","hash":"3f320db2922fbe060fc9df845df15aa5c6311dee","modified":1654144644304},{"_id":"public/2022/06/01/论文导读：使用松弛半定规划的方法进行数据降维--通过最大最小距离/index.html","hash":"813c2499eccf222827d0dd8ea7efeb56fc3bd21e","modified":1654144644304},{"_id":"public/archives/index.html","hash":"3a0c138dea8b529d6e56a158dd655c136ecc7be4","modified":1654144644304},{"_id":"public/index.html","hash":"f814045e8d731e701cc098acf9941fb819e195e2","modified":1654144644304},{"_id":"public/archives/2022/index.html","hash":"3a0c138dea8b529d6e56a158dd655c136ecc7be4","modified":1654144644304},{"_id":"public/archives/2022/06/index.html","hash":"3a0c138dea8b529d6e56a158dd655c136ecc7be4","modified":1654144644304},{"_id":"public/categories/信号/index.html","hash":"443030b299d17bcf600b204d41cb09ff404a430b","modified":1654144644304},{"_id":"public/images/12.jpg","hash":"88d68b169744a76ca7b21a258fc0812bc03ee701","modified":1654144644304},{"_id":"public/images/3123.jpg","hash":"fc005184ff616231647c5dc85c63167040d810ba","modified":1654144644304},{"_id":"public/images/overlay.png","hash":"e978d81201508e6fcbe0fe95485dae1a4b983a8d","modified":1654144644304},{"_id":"public/images/pic03.jpg","hash":"ec46b8b1490a794ea5d4c61058959b9a52ac262f","modified":1654144644304},{"_id":"public/images/pic02.jpg","hash":"ab77495fef5102f94e36b71d4308cd682dd4b1a8","modified":1654144644304},{"_id":"public/images/pic04.jpg","hash":"ee299cd40108e50d7151333de525049f8ec4f150","modified":1654144644304},{"_id":"public/images/pic05.jpg","hash":"b74d638c0aa9e90f94b056db4c0a886e382973f1","modified":1654144644304},{"_id":"public/images/pic01.jpg","hash":"774db644cdac607e0d9d72ca65a4961f9a250c11","modified":1654144644304},{"_id":"public/images/pic06.jpg","hash":"3d8572663edf8a77b99bdfd93491d4f6a7298aaf","modified":1654144644304},{"_id":"public/images/pic08.jpg","hash":"993e8ff91c20b6d28b6302d3a18367547610c58a","modified":1654144644304},{"_id":"public/images/pic07.jpg","hash":"de06b4eee3d0e42ddcac415adc8c83466f430253","modified":1654144644304},{"_id":"public/images/pic11.jpg","hash":"a034beb80041d238f36685706a019a269f159ac1","modified":1654144644304},{"_id":"public/images/pic09.jpg","hash":"6aae76e8215e3a6e785ce2a65484e4df03ed5fe9","modified":1654144644304},{"_id":"public/images/pic12.jpg","hash":"ae93f2674a3b1f1dcbfe9692f2e172e106c02d96","modified":1654144644304},{"_id":"public/images/picx03.jpg","hash":"cffaa38c9489f131f6f37763b37b937811b316df","modified":1654144644304},{"_id":"public/images/pic10.jpg","hash":"814a661f2966a3c8fe2c89034f62e828f291d770","modified":1654144644304},{"_id":"public/images/呗.jpg","hash":"b520aa4d797fc0794d93104f260d18a523b67522","modified":1654144644304},{"_id":"public/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1654144644304},{"_id":"public/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1654144644304},{"_id":"public/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1654144644304},{"_id":"public/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1654144644304},{"_id":"public/images/logo.png","hash":"ee2cf634d838f8acab5b826400edb0b42c8ce52b","modified":1654144644304},{"_id":"public/css/images/arrow.svg","hash":"84dba9fe4dffab02e5e32f66cea1e38c12129a7c","modified":1654144644304},{"_id":"public/css/images/arrow-small.svg","hash":"ecc2016f117d725f156bdb60a81e4d09d88961e1","modified":1654144644304},{"_id":"public/css/images/close.svg","hash":"205739dd10d087f013583087e012ee7423a99c34","modified":1654144644304},{"_id":"public/css/images/close-small.svg","hash":"92f0c906a7540f0bac4e4aad1387362f218ee02f","modified":1654144644304},{"_id":"public/css/images/close-small-alt.svg","hash":"7908654ee26cd247d52fb4fae6ad6b8060a00598","modified":1654144644304},{"_id":"public/css/images/spinner.svg","hash":"285d4c263d37d920d65a1c67c870a05dbbdbb7c3","modified":1654144644304},{"_id":"public/css/images/open-small.svg","hash":"068e6e2c79172fc81d8f1471750fd6f0658c682f","modified":1654144644304},{"_id":"public/images/fulls/01.jpg","hash":"cfc928948301b97ab2873e837284cd83cc3c6cff","modified":1654144644304},{"_id":"public/css/images/open.svg","hash":"5e61408876123f8a53a1491899b1c813934c299b","modified":1654144644304},{"_id":"public/images/fulls/02.jpg","hash":"1a729f597316a7483ebcf39b5984ced1612a0410","modified":1654144644304},{"_id":"public/images/fulls/03.jpg","hash":"0f8c8109f8b605aa46ef571df1d16e8bdd43d090","modified":1654144644304},{"_id":"public/images/fulls/04.jpg","hash":"806679d4c4398335413b6b53941709a4aef7c3cd","modified":1654144644304},{"_id":"public/images/fulls/05.jpg","hash":"232b0b9e3b6e751ebb2e6c4284651639526200e3","modified":1654144644304},{"_id":"public/images/fulls/06.jpg","hash":"e97b29127a50c69cf445d0591cd94daab0f50e33","modified":1654144644304},{"_id":"public/images/fulls/08.jpg","hash":"d31c389218f04bc7ec510ac236fbe343048e3cfc","modified":1654144644304},{"_id":"public/images/fulls/07.jpg","hash":"eb41675450650b613195806b154c6badbb7eca4d","modified":1654144644304},{"_id":"public/images/fulls/09.jpg","hash":"61fab3aa8ceee938efb4965042ce2b5f51d3a584","modified":1654144644304},{"_id":"public/images/fulls/10.jpg","hash":"9a284568752a54ab9e8e9cd19c41750e05975d2a","modified":1654144644304},{"_id":"public/images/fulls/11.jpg","hash":"cff6219163e3154c1263ee1d7f924c0ae3ccc9e7","modified":1654144644304},{"_id":"public/images/fulls/12.jpg","hash":"a631491537584e2e23926d9d6f50ce60bb884b18","modified":1654144644304},{"_id":"public/images/thumbs/01.jpg","hash":"4f5a2b9fdf44146d60f8dd3a32c81a8419b9148f","modified":1654144644304},{"_id":"public/images/thumbs/02.jpg","hash":"396a422cbc1b94b6c9b00f6a34c02b05913230ba","modified":1654144644304},{"_id":"public/images/thumbs/04.jpg","hash":"3c60445217b1b52b1fd6cae4578356c564d7624d","modified":1654144644304},{"_id":"public/images/thumbs/03.jpg","hash":"e3915b1cc3283b5ec33f49e2591ebf2e6be2362e","modified":1654144644304},{"_id":"public/images/thumbs/05.jpg","hash":"3e28c2253db93ba42d16ba9a1f451dbc452fd947","modified":1654144644304},{"_id":"public/images/thumbs/06.jpg","hash":"ffc80bb5fe28626d9462e06d9f01d2a5f99b0ae9","modified":1654144644304},{"_id":"public/images/thumbs/09.jpg","hash":"bc8a02decf65e44657f02dcc11c7395bfa41ffdb","modified":1654144644304},{"_id":"public/images/thumbs/07.jpg","hash":"45194a98089345e4f135c0f0bfffeee7ccdd9c99","modified":1654144644304},{"_id":"public/images/thumbs/08.jpg","hash":"e0384120f6acfa64ad0b5bd0d7e114e9f239294e","modified":1654144644304},{"_id":"public/images/thumbs/10.jpg","hash":"ccfee49fbca187bad971336cb8fda7dec05ab01e","modified":1654144644304},{"_id":"public/css/demo.css","hash":"99b020fe28a276c2b128a5048ab1325fc3303b96","modified":1654144644304},{"_id":"public/css/dropdownMenu.css","hash":"3e44ce25f19c28dcf2c12db1938ccce7a081c10f","modified":1654144644304},{"_id":"public/css/backcss.css","hash":"65db154a14ccb0e773a14e1d1262553e6f30aec5","modified":1654144644304},{"_id":"public/css/my_link.css","hash":"c9535592792f2271df88c39e4baf8c849e23db17","modified":1654144644304},{"_id":"public/css/normalize.css","hash":"a0ff1e2feee0b1bb7e54a88fd6367ba3e7435aaf","modified":1654144644304},{"_id":"public/css/prism.css","hash":"b39768264f9923a770bcc3f289be0a2bd8e64268","modified":1654144644304},{"_id":"public/css/prism_coy.css","hash":"24ebf5f3e5f68341522204dcaaf33f429d3c3718","modified":1654144644304},{"_id":"public/css/noscript.css","hash":"5579ccdecc63139609db824704331d8d4af61538","modified":1654144644304},{"_id":"public/css/prism_dark.css","hash":"8085542cdb4583ffb78b2edd146ebe9511bf668c","modified":1654144644304},{"_id":"public/css/prism_default.css","hash":"3600bee175b309662e306b50e494ac73495619c2","modified":1654144644304},{"_id":"public/css/prism_funky.css","hash":"aa0e3a37fb8ccce44457d7805a13d7c6094d1f5e","modified":1654144644304},{"_id":"public/css/prism_okaidia.css","hash":"e20586e80aeef3192c727e20d2efe4bd23846d29","modified":1654144644304},{"_id":"public/css/prism_solarized.css","hash":"73759f72dc43870aaf76dca7ef4e61a142b6b4f0","modified":1654144644304},{"_id":"public/css/ok_prism.css","hash":"e20586e80aeef3192c727e20d2efe4bd23846d29","modified":1654144644304},{"_id":"public/css/prism_twilight.css","hash":"17498435f6d964243c1a50deb563e6003aebe0e6","modified":1654144644304},{"_id":"public/css/typo.css","hash":"768004dae4204dfbda865d412229feaa618ec3b7","modified":1654144644304},{"_id":"public/js/gallery_skel.min.js","hash":"475a99682e46ad061915a11a9adb3fad82258d3c","modified":1654144644304},{"_id":"public/js/jquery.scrollex.min.js","hash":"57fa1d1de2eca4fabbe75d6d160edc45cc398075","modified":1654144644304},{"_id":"public/js/main.js","hash":"509935fbbf5896ee1e8f2047e961dd58894ae74d","modified":1654144644304},{"_id":"public/js/lazyload.min.js","hash":"5348fd7aa4dbefac9d21091c9fd5e263563b5540","modified":1654144644304},{"_id":"public/js/mic_gallery.js","hash":"9f5d540f90fcef501df7a9f8272203d1604cd507","modified":1654144644304},{"_id":"public/js/jquery.scrolly.min.js","hash":"6807fda8e44d95117e7007563a4db951818df82b","modified":1654144644304},{"_id":"public/js/skel.min.js","hash":"475a99682e46ad061915a11a9adb3fad82258d3c","modified":1654144644304},{"_id":"public/js/util.js","hash":"3ece9010adc07dd3dc27ced0e22e7ac8bcd16e14","modified":1654144644304},{"_id":"public/css/gallery.min.css","hash":"c5333ca835aae49239d809130a7b281af745729e","modified":1654144644304},{"_id":"public/css/font-awesome.min.css","hash":"03eb0a6b53db2655f3ded9bb1a4e4af07cc1efa6","modified":1654144644304},{"_id":"public/css/gallery.css","hash":"4c0178ea898cdc5e010346b56d1c9845fe8da53e","modified":1654144644304},{"_id":"public/css/item.css","hash":"da1277164b611ccba56beda47960f9075c61dbab","modified":1654144644304},{"_id":"public/css/mic_gallery.css","hash":"5c75b8ab2670599538a1250957cf7c44d0e4cbec","modified":1654144644304},{"_id":"public/css/mic_main.css","hash":"152cca3d857f0184a99d1e8ac54dcb1d19945b3e","modified":1654144644304},{"_id":"public/js/jquery.min.js","hash":"276c87ff3e1e3155679c318938e74e5c1b76d809","modified":1654144644304},{"_id":"public/js/prism.js","hash":"0868d642f4661649a37dfb45b93fa30b4d92d4e4","modified":1654144644304},{"_id":"public/images/thumbs/11.jpg","hash":"66a7fb237824b1b2fd550a068bb6a32f5031ab8f","modified":1654144644304},{"_id":"public/images/thumbs/12.jpg","hash":"17ab685447ed06b6c12e4d42f38111c684221bc7","modified":1654144644304},{"_id":"public/img/logo_miccall.png","hash":"ee2cf634d838f8acab5b826400edb0b42c8ce52b","modified":1654144644304},{"_id":"public/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1654144644304},{"_id":"public/images/me.jpg","hash":"853d6623c9bbde9ed66855a27782f1e01f7f24d5","modified":1654144644304},{"_id":"public/js/gallery/gallery.js","hash":"8ee48312a183b42a9886211a0ec825ea0d041301","modified":1654144644304},{"_id":"public/fonts/fontawesome-webfont.svg","hash":"964e8dac2cee79c4b49b5bc300675773f6518812","modified":1654144644304},{"_id":"public/images/21.jpg","hash":"0b2570df0e6d9b79a0c50d0b4ce8799bd40ea421","modified":1654144644304},{"_id":"public/img/ba.jpg","hash":"af7fe6067d0c21f558502613eed86d8246000bba","modified":1654144644304},{"_id":"public/img/bg.jpg","hash":"76c3942791aef649ebd4776b5f768c9f2a881400","modified":1654144644304},{"_id":"public/fonts/FZLT.ttf","hash":"24dfab81236612d596c97eb38b3adf5de99f669c","modified":1654144644304}],"Category":[{"name":"信号","_id":"cl3wj0gdr0002gsw1a3zm4drc"},{"name":"编程","_id":"cl3wj0gdw0007gsw16jqsegb3"}],"Data":[],"Page":[],"Post":[{"title":"MCMC系列：Metropololis-Hasting采样算法，10分钟从零到大神","date":"2022-06-01T16:00:00.000Z","thumbnail":"https://tva1.sinaimg.cn/mw690/94aee95bgy1h2tsjem77mj21hc0u0hdt.jpg","mathjax":true,"_content":"# MCMC\n\n面对已知的复杂分布时，仅知其表达式，对其进行采样变得格外棘手（甚至无法采样）。在仿真中，各工具包会提供一些标准分布，MCMC采样方法允许我们通过构造一个马尔科夫链，并借助标准分布，使得以某种接受概率采样的新马尔科夫链的平稳分布为我们的目标复杂分布。\n\n## Metropolis-Hasting\n\nMCMC中很重要且很实用的一个采样算法就是MH算法。\n\n### MH算法流程(怎么做)\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/da80fe5bb4c94854b8a02a35b5cb1702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n\n\n\n\n### 这么做为什么行(MH推导)\n\n#### 从马尔可夫链谈起(重点版)\n\n如何构建一个平稳分布为目标分布马尔科夫链？可在此简要介绍会用到的马尔可夫链的重要性质(假设读者了解马尔科夫链的基本概念)：\n\n*稳态分布*  **什么是稳态分布呢(stationary/invariant distribution)?** 定义$(X_n)_{n\\geq 0}$为马氏链$(\\lambda,P)$。当马尔可夫链$(X_n)_{n\\geq 0}$的状态分布为$\\lambda$时，此时若$\\pi$满足\n$$\\pi P=\\pi$$\n那么$pi$则是该马尔科夫链的平稳分布。通俗理解即为某一时刻马尔科夫链的状态服从某一个分布之后，再进行转移，其状态仍然服从此分布(服从分布不是状态保持不变)。至此，已经阐明为何可以使用马尔可夫链进行采样。\n\n但是如何找到平稳分布呢？MH中利用了马尔科夫链的细致平衡方程(detailed\nbalance equations),满足细致平衡方程的的分布就是马氏链的平稳分布。\n\n\n**定理 1.1**. \n*如果存在一个分布$\\pi$满足$$\\pi_i P_{ij} = \\pi_j P_{ji},$$ 那么$\\pi$就是该马氏链的平稳分布。*\n\n\n**Proof.** *$$\\sum_{i} \\pi_i P_{ij}=\\sum_{i}\\pi_j P_{ji}=\\pi_j$$ ◻*\n\n#### 回到MH算法（Metropolis Hastings algorithm） \n\n介绍完马氏链的平稳分布后，可以发现，MH算法就是在构造马氏链满足细致平稳条件的转移概率，使其平稳分布为我们的目标分布。在中的第5行构造了转移的接受概率，马氏链的转移分布由提议分布$q$决定，此时$P_{ij}=q(x_j|x_i)$。考虑$\\alpha_{ij}<1$的情况，有\n$$p_i Q_{ij} = p_i P_{ij} \\alpha_{ij} =p_i q(x_j|x_i) \\frac{p_jq(x_i|x_j)}{p_i q(x_j|x_i)}=p_jq(x_i|x_j)\\alpha_{ji}=p_j Q_{ji}$$\n可以发现这样构造出来的新马氏链(实际上叫做MH链或者MH马氏链)的平稳分布就是需要被采样的目标分布了。那么对这个MH链采样一段时间后(burn\nin)，MH链会收敛到平稳分布。\n\n#### 问题再现，再次回到马氏链\n\n读者不免会产生一定的疑问:对于一个马氏链来说平稳分布一定存在且唯一吗？会不会存在一个马氏链有多个平稳分布的情况呢？老规矩，先说结论：对于有限空间的马尔科夫链，平稳分布一定存在，对于不可约的(状态互通)马尔科夫链，平稳分布就是唯一的。\n\n\n**Proof.** \n**声明！本证明已掏空笔者！！**\n首先解释何为**不可约(状态互通)**，即状态之间能够互通。从状态$i$经过$n$步之后可以到达状态$j$，即$Pr(x_n=j|x_0=i,n<-\\infty)>0$。状态矩阵$P^n$中的每一个元素都大于$0$，所以，不可约马氏链的状态矩阵是一个正则矩阵(regular matrix)。\\[*$A$ is called regular if for some $k>1$,$A^k>0$.*\\]根据Perron-Frobenius定理，若一个矩阵$A$是正则矩阵，则\n\n1.  $A$存在一个正实特征值$\\lambda_{PF}$，其对应的左、右特征向量也均为正特征向量。\n\n2.  $A$的所有其他特征值都存在$\\lambda_i < \\lambda_{PF}$。\n\n3.  $\\lambda_{PF}=1$，且其左、右特征向量的模为$1$。\n\n那么马氏链的转移矩阵$P$只有一个特征向量$\\pi$能够满足\n$$\\pi P=1\\cdot\\pi$$ ◻\n证明了不可约的马氏链只一个单独的平稳分布之后，就容易理解为什么满足细致平衡方程的分布就是批评翁分布。至此，MH为什么可行就得到了证明。\n\n#### 真的结束了吗？补充证明\n\n真的结束了吗。不妨思考，需要被采样的空间是一个有限空间吗？对目标分布进行采样，明明可以得到无数的采样值。这样之前证明的前提便全部被推翻了呀！\n在此，引入新的概念，马尔科夫链的另一种分类，countable-state\nchain，旨在描述类似连续分布采样过程的离散时间无限空间的马尔科夫链。那无限空间的马氏链仍然只有唯一的平稳分布吗。结论：对于不可约的马尔科夫链，如果$\\pi P=\\pi$有解，那么这个解是唯一的。相反，如果马氏链是正常反的，那么就一定有解。在此不对上述结论进行证明，有意者可以参考\\<Stochastic\nProcesses:Theory for Applications>的第298页的定理6.3.8.\n","source":"_posts/MCMC系列：Metropololis-Hasting采样算法，10分钟从零到大神.md","raw":"---\ntitle: MCMC系列：Metropololis-Hasting采样算法，10分钟从零到大神\ndate: 2022-06-02\ntags: 统计学\ncategories: 信号\nthumbnail: https://tva1.sinaimg.cn/mw690/94aee95bgy1h2tsjem77mj21hc0u0hdt.jpg\nmathjax: true\n---\n# MCMC\n\n面对已知的复杂分布时，仅知其表达式，对其进行采样变得格外棘手（甚至无法采样）。在仿真中，各工具包会提供一些标准分布，MCMC采样方法允许我们通过构造一个马尔科夫链，并借助标准分布，使得以某种接受概率采样的新马尔科夫链的平稳分布为我们的目标复杂分布。\n\n## Metropolis-Hasting\n\nMCMC中很重要且很实用的一个采样算法就是MH算法。\n\n### MH算法流程(怎么做)\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/da80fe5bb4c94854b8a02a35b5cb1702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n\n\n\n\n### 这么做为什么行(MH推导)\n\n#### 从马尔可夫链谈起(重点版)\n\n如何构建一个平稳分布为目标分布马尔科夫链？可在此简要介绍会用到的马尔可夫链的重要性质(假设读者了解马尔科夫链的基本概念)：\n\n*稳态分布*  **什么是稳态分布呢(stationary/invariant distribution)?** 定义$(X_n)_{n\\geq 0}$为马氏链$(\\lambda,P)$。当马尔可夫链$(X_n)_{n\\geq 0}$的状态分布为$\\lambda$时，此时若$\\pi$满足\n$$\\pi P=\\pi$$\n那么$pi$则是该马尔科夫链的平稳分布。通俗理解即为某一时刻马尔科夫链的状态服从某一个分布之后，再进行转移，其状态仍然服从此分布(服从分布不是状态保持不变)。至此，已经阐明为何可以使用马尔可夫链进行采样。\n\n但是如何找到平稳分布呢？MH中利用了马尔科夫链的细致平衡方程(detailed\nbalance equations),满足细致平衡方程的的分布就是马氏链的平稳分布。\n\n\n**定理 1.1**. \n*如果存在一个分布$\\pi$满足$$\\pi_i P_{ij} = \\pi_j P_{ji},$$ 那么$\\pi$就是该马氏链的平稳分布。*\n\n\n**Proof.** *$$\\sum_{i} \\pi_i P_{ij}=\\sum_{i}\\pi_j P_{ji}=\\pi_j$$ ◻*\n\n#### 回到MH算法（Metropolis Hastings algorithm） \n\n介绍完马氏链的平稳分布后，可以发现，MH算法就是在构造马氏链满足细致平稳条件的转移概率，使其平稳分布为我们的目标分布。在中的第5行构造了转移的接受概率，马氏链的转移分布由提议分布$q$决定，此时$P_{ij}=q(x_j|x_i)$。考虑$\\alpha_{ij}<1$的情况，有\n$$p_i Q_{ij} = p_i P_{ij} \\alpha_{ij} =p_i q(x_j|x_i) \\frac{p_jq(x_i|x_j)}{p_i q(x_j|x_i)}=p_jq(x_i|x_j)\\alpha_{ji}=p_j Q_{ji}$$\n可以发现这样构造出来的新马氏链(实际上叫做MH链或者MH马氏链)的平稳分布就是需要被采样的目标分布了。那么对这个MH链采样一段时间后(burn\nin)，MH链会收敛到平稳分布。\n\n#### 问题再现，再次回到马氏链\n\n读者不免会产生一定的疑问:对于一个马氏链来说平稳分布一定存在且唯一吗？会不会存在一个马氏链有多个平稳分布的情况呢？老规矩，先说结论：对于有限空间的马尔科夫链，平稳分布一定存在，对于不可约的(状态互通)马尔科夫链，平稳分布就是唯一的。\n\n\n**Proof.** \n**声明！本证明已掏空笔者！！**\n首先解释何为**不可约(状态互通)**，即状态之间能够互通。从状态$i$经过$n$步之后可以到达状态$j$，即$Pr(x_n=j|x_0=i,n<-\\infty)>0$。状态矩阵$P^n$中的每一个元素都大于$0$，所以，不可约马氏链的状态矩阵是一个正则矩阵(regular matrix)。\\[*$A$ is called regular if for some $k>1$,$A^k>0$.*\\]根据Perron-Frobenius定理，若一个矩阵$A$是正则矩阵，则\n\n1.  $A$存在一个正实特征值$\\lambda_{PF}$，其对应的左、右特征向量也均为正特征向量。\n\n2.  $A$的所有其他特征值都存在$\\lambda_i < \\lambda_{PF}$。\n\n3.  $\\lambda_{PF}=1$，且其左、右特征向量的模为$1$。\n\n那么马氏链的转移矩阵$P$只有一个特征向量$\\pi$能够满足\n$$\\pi P=1\\cdot\\pi$$ ◻\n证明了不可约的马氏链只一个单独的平稳分布之后，就容易理解为什么满足细致平衡方程的分布就是批评翁分布。至此，MH为什么可行就得到了证明。\n\n#### 真的结束了吗？补充证明\n\n真的结束了吗。不妨思考，需要被采样的空间是一个有限空间吗？对目标分布进行采样，明明可以得到无数的采样值。这样之前证明的前提便全部被推翻了呀！\n在此，引入新的概念，马尔科夫链的另一种分类，countable-state\nchain，旨在描述类似连续分布采样过程的离散时间无限空间的马尔科夫链。那无限空间的马氏链仍然只有唯一的平稳分布吗。结论：对于不可约的马尔科夫链，如果$\\pi P=\\pi$有解，那么这个解是唯一的。相反，如果马氏链是正常反的，那么就一定有解。在此不对上述结论进行证明，有意者可以参考\\<Stochastic\nProcesses:Theory for Applications>的第298页的定理6.3.8.\n","slug":"MCMC系列：Metropololis-Hasting采样算法，10分钟从零到大神","published":1,"updated":"2022-06-02T04:34:17.007Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdf0000gsw107vl0y5e","content":"<h1 id=\"MCMC\"><a href=\"#MCMC\" class=\"headerlink\" title=\"MCMC\"></a>MCMC</h1><p>面对已知的复杂分布时，仅知其表达式，对其进行采样变得格外棘手（甚至无法采样）。在仿真中，各工具包会提供一些标准分布，MCMC采样方法允许我们通过构造一个马尔科夫链，并借助标准分布，使得以某种接受概率采样的新马尔科夫链的平稳分布为我们的目标复杂分布。</p>\n<h2 id=\"Metropolis-Hasting\"><a href=\"#Metropolis-Hasting\" class=\"headerlink\" title=\"Metropolis-Hasting\"></a>Metropolis-Hasting</h2><p>MCMC中很重要且很实用的一个采样算法就是MH算法。</p>\n<h3 id=\"MH算法流程-怎么做\"><a href=\"#MH算法流程-怎么做\" class=\"headerlink\" title=\"MH算法流程(怎么做)\"></a>MH算法流程(怎么做)</h3><p><img src=\"https://img-blog.csdnimg.cn/da80fe5bb4c94854b8a02a35b5cb1702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"这么做为什么行-MH推导\"><a href=\"#这么做为什么行-MH推导\" class=\"headerlink\" title=\"这么做为什么行(MH推导)\"></a>这么做为什么行(MH推导)</h3><h4 id=\"从马尔可夫链谈起-重点版\"><a href=\"#从马尔可夫链谈起-重点版\" class=\"headerlink\" title=\"从马尔可夫链谈起(重点版)\"></a>从马尔可夫链谈起(重点版)</h4><p>如何构建一个平稳分布为目标分布马尔科夫链？可在此简要介绍会用到的马尔可夫链的重要性质(假设读者了解马尔科夫链的基本概念)：</p>\n<p><em>稳态分布</em>  <strong>什么是稳态分布呢(stationary/invariant distribution)?</strong> 定义$(X_n)_{n\\geq 0}$为马氏链$(\\lambda,P)$。当马尔可夫链$(X_n)_{n\\geq 0}$的状态分布为$\\lambda$时，此时若$\\pi$满足</p>\n<script type=\"math/tex; mode=display\">\\pi P=\\pi</script><p>那么$pi$则是该马尔科夫链的平稳分布。通俗理解即为某一时刻马尔科夫链的状态服从某一个分布之后，再进行转移，其状态仍然服从此分布(服从分布不是状态保持不变)。至此，已经阐明为何可以使用马尔可夫链进行采样。</p>\n<p>但是如何找到平稳分布呢？MH中利用了马尔科夫链的细致平衡方程(detailed<br>balance equations),满足细致平衡方程的的分布就是马氏链的平稳分布。</p>\n<p><strong>定理 1.1</strong>.<br><em>如果存在一个分布$\\pi$满足<script type=\"math/tex\">\\pi_i P_{ij} = \\pi_j P_{ji},</script> 那么$\\pi$就是该马氏链的平稳分布。</em></p>\n<p><strong>Proof.</strong> <em><script type=\"math/tex\">\\sum_{i} \\pi_i P_{ij}=\\sum_{i}\\pi_j P_{ji}=\\pi_j</script> ◻</em></p>\n<h4 id=\"回到MH算法（Metropolis-Hastings-algorithm）\"><a href=\"#回到MH算法（Metropolis-Hastings-algorithm）\" class=\"headerlink\" title=\"回到MH算法（Metropolis Hastings algorithm）\"></a>回到MH算法（Metropolis Hastings algorithm）</h4><p>介绍完马氏链的平稳分布后，可以发现，MH算法就是在构造马氏链满足细致平稳条件的转移概率，使其平稳分布为我们的目标分布。在中的第5行构造了转移的接受概率，马氏链的转移分布由提议分布$q$决定，此时$P_{ij}=q(x_j|x_i)$。考虑$\\alpha_{ij}&lt;1$的情况，有</p>\n<script type=\"math/tex; mode=display\">p_i Q_{ij} = p_i P_{ij} \\alpha_{ij} =p_i q(x_j|x_i) \\frac{p_jq(x_i|x_j)}{p_i q(x_j|x_i)}=p_jq(x_i|x_j)\\alpha_{ji}=p_j Q_{ji}</script><p>可以发现这样构造出来的新马氏链(实际上叫做MH链或者MH马氏链)的平稳分布就是需要被采样的目标分布了。那么对这个MH链采样一段时间后(burn<br>in)，MH链会收敛到平稳分布。</p>\n<h4 id=\"问题再现，再次回到马氏链\"><a href=\"#问题再现，再次回到马氏链\" class=\"headerlink\" title=\"问题再现，再次回到马氏链\"></a>问题再现，再次回到马氏链</h4><p>读者不免会产生一定的疑问:对于一个马氏链来说平稳分布一定存在且唯一吗？会不会存在一个马氏链有多个平稳分布的情况呢？老规矩，先说结论：对于有限空间的马尔科夫链，平稳分布一定存在，对于不可约的(状态互通)马尔科夫链，平稳分布就是唯一的。</p>\n<p><strong>Proof.</strong><br><strong>声明！本证明已掏空笔者！！</strong><br>首先解释何为<strong>不可约(状态互通)</strong>，即状态之间能够互通。从状态$i$经过$n$步之后可以到达状态$j$，即$Pr(x_n=j|x_0=i,n&lt;-\\infty)&gt;0$。状态矩阵$P^n$中的每一个元素都大于$0$，所以，不可约马氏链的状态矩阵是一个正则矩阵(regular matrix)。[<em>$A$ is called regular if for some $k&gt;1$,$A^k&gt;0$.</em>]根据Perron-Frobenius定理，若一个矩阵$A$是正则矩阵，则</p>\n<ol>\n<li><p>$A$存在一个正实特征值$\\lambda_{PF}$，其对应的左、右特征向量也均为正特征向量。</p>\n</li>\n<li><p>$A$的所有其他特征值都存在$\\lambda_i &lt; \\lambda_{PF}$。</p>\n</li>\n<li><p>$\\lambda_{PF}=1$，且其左、右特征向量的模为$1$。</p>\n</li>\n</ol>\n<p>那么马氏链的转移矩阵$P$只有一个特征向量$\\pi$能够满足</p>\n<p><script type=\"math/tex\">\\pi P=1\\cdot\\pi</script> ◻<br>证明了不可约的马氏链只一个单独的平稳分布之后，就容易理解为什么满足细致平衡方程的分布就是批评翁分布。至此，MH为什么可行就得到了证明。</p>\n<h4 id=\"真的结束了吗？补充证明\"><a href=\"#真的结束了吗？补充证明\" class=\"headerlink\" title=\"真的结束了吗？补充证明\"></a>真的结束了吗？补充证明</h4><p>真的结束了吗。不妨思考，需要被采样的空间是一个有限空间吗？对目标分布进行采样，明明可以得到无数的采样值。这样之前证明的前提便全部被推翻了呀！<br>在此，引入新的概念，马尔科夫链的另一种分类，countable-state<br>chain，旨在描述类似连续分布采样过程的离散时间无限空间的马尔科夫链。那无限空间的马氏链仍然只有唯一的平稳分布吗。结论：对于不可约的马尔科夫链，如果$\\pi P=\\pi$有解，那么这个解是唯一的。相反，如果马氏链是正常反的，那么就一定有解。在此不对上述结论进行证明，有意者可以参考\\<Stochastic\nProcesses:Theory for Applications>的第298页的定理6.3.8.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"MCMC\"><a href=\"#MCMC\" class=\"headerlink\" title=\"MCMC\"></a>MCMC</h1><p>面对已知的复杂分布时，仅知其表达式，对其进行采样变得格外棘手（甚至无法采样）。在仿真中，各工具包会提供一些标准分布，MCMC采样方法允许我们通过构造一个马尔科夫链，并借助标准分布，使得以某种接受概率采样的新马尔科夫链的平稳分布为我们的目标复杂分布。</p>\n<h2 id=\"Metropolis-Hasting\"><a href=\"#Metropolis-Hasting\" class=\"headerlink\" title=\"Metropolis-Hasting\"></a>Metropolis-Hasting</h2><p>MCMC中很重要且很实用的一个采样算法就是MH算法。</p>\n<h3 id=\"MH算法流程-怎么做\"><a href=\"#MH算法流程-怎么做\" class=\"headerlink\" title=\"MH算法流程(怎么做)\"></a>MH算法流程(怎么做)</h3><p><img src=\"https://img-blog.csdnimg.cn/da80fe5bb4c94854b8a02a35b5cb1702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"这么做为什么行-MH推导\"><a href=\"#这么做为什么行-MH推导\" class=\"headerlink\" title=\"这么做为什么行(MH推导)\"></a>这么做为什么行(MH推导)</h3><h4 id=\"从马尔可夫链谈起-重点版\"><a href=\"#从马尔可夫链谈起-重点版\" class=\"headerlink\" title=\"从马尔可夫链谈起(重点版)\"></a>从马尔可夫链谈起(重点版)</h4><p>如何构建一个平稳分布为目标分布马尔科夫链？可在此简要介绍会用到的马尔可夫链的重要性质(假设读者了解马尔科夫链的基本概念)：</p>\n<p><em>稳态分布</em>  <strong>什么是稳态分布呢(stationary/invariant distribution)?</strong> 定义$(X_n)_{n\\geq 0}$为马氏链$(\\lambda,P)$。当马尔可夫链$(X_n)_{n\\geq 0}$的状态分布为$\\lambda$时，此时若$\\pi$满足</p>\n<script type=\"math/tex; mode=display\">\\pi P=\\pi</script><p>那么$pi$则是该马尔科夫链的平稳分布。通俗理解即为某一时刻马尔科夫链的状态服从某一个分布之后，再进行转移，其状态仍然服从此分布(服从分布不是状态保持不变)。至此，已经阐明为何可以使用马尔可夫链进行采样。</p>\n<p>但是如何找到平稳分布呢？MH中利用了马尔科夫链的细致平衡方程(detailed<br>balance equations),满足细致平衡方程的的分布就是马氏链的平稳分布。</p>\n<p><strong>定理 1.1</strong>.<br><em>如果存在一个分布$\\pi$满足<script type=\"math/tex\">\\pi_i P_{ij} = \\pi_j P_{ji},</script> 那么$\\pi$就是该马氏链的平稳分布。</em></p>\n<p><strong>Proof.</strong> <em><script type=\"math/tex\">\\sum_{i} \\pi_i P_{ij}=\\sum_{i}\\pi_j P_{ji}=\\pi_j</script> ◻</em></p>\n<h4 id=\"回到MH算法（Metropolis-Hastings-algorithm）\"><a href=\"#回到MH算法（Metropolis-Hastings-algorithm）\" class=\"headerlink\" title=\"回到MH算法（Metropolis Hastings algorithm）\"></a>回到MH算法（Metropolis Hastings algorithm）</h4><p>介绍完马氏链的平稳分布后，可以发现，MH算法就是在构造马氏链满足细致平稳条件的转移概率，使其平稳分布为我们的目标分布。在中的第5行构造了转移的接受概率，马氏链的转移分布由提议分布$q$决定，此时$P_{ij}=q(x_j|x_i)$。考虑$\\alpha_{ij}&lt;1$的情况，有</p>\n<script type=\"math/tex; mode=display\">p_i Q_{ij} = p_i P_{ij} \\alpha_{ij} =p_i q(x_j|x_i) \\frac{p_jq(x_i|x_j)}{p_i q(x_j|x_i)}=p_jq(x_i|x_j)\\alpha_{ji}=p_j Q_{ji}</script><p>可以发现这样构造出来的新马氏链(实际上叫做MH链或者MH马氏链)的平稳分布就是需要被采样的目标分布了。那么对这个MH链采样一段时间后(burn<br>in)，MH链会收敛到平稳分布。</p>\n<h4 id=\"问题再现，再次回到马氏链\"><a href=\"#问题再现，再次回到马氏链\" class=\"headerlink\" title=\"问题再现，再次回到马氏链\"></a>问题再现，再次回到马氏链</h4><p>读者不免会产生一定的疑问:对于一个马氏链来说平稳分布一定存在且唯一吗？会不会存在一个马氏链有多个平稳分布的情况呢？老规矩，先说结论：对于有限空间的马尔科夫链，平稳分布一定存在，对于不可约的(状态互通)马尔科夫链，平稳分布就是唯一的。</p>\n<p><strong>Proof.</strong><br><strong>声明！本证明已掏空笔者！！</strong><br>首先解释何为<strong>不可约(状态互通)</strong>，即状态之间能够互通。从状态$i$经过$n$步之后可以到达状态$j$，即$Pr(x_n=j|x_0=i,n&lt;-\\infty)&gt;0$。状态矩阵$P^n$中的每一个元素都大于$0$，所以，不可约马氏链的状态矩阵是一个正则矩阵(regular matrix)。[<em>$A$ is called regular if for some $k&gt;1$,$A^k&gt;0$.</em>]根据Perron-Frobenius定理，若一个矩阵$A$是正则矩阵，则</p>\n<ol>\n<li><p>$A$存在一个正实特征值$\\lambda_{PF}$，其对应的左、右特征向量也均为正特征向量。</p>\n</li>\n<li><p>$A$的所有其他特征值都存在$\\lambda_i &lt; \\lambda_{PF}$。</p>\n</li>\n<li><p>$\\lambda_{PF}=1$，且其左、右特征向量的模为$1$。</p>\n</li>\n</ol>\n<p>那么马氏链的转移矩阵$P$只有一个特征向量$\\pi$能够满足</p>\n<p><script type=\"math/tex\">\\pi P=1\\cdot\\pi</script> ◻<br>证明了不可约的马氏链只一个单独的平稳分布之后，就容易理解为什么满足细致平衡方程的分布就是批评翁分布。至此，MH为什么可行就得到了证明。</p>\n<h4 id=\"真的结束了吗？补充证明\"><a href=\"#真的结束了吗？补充证明\" class=\"headerlink\" title=\"真的结束了吗？补充证明\"></a>真的结束了吗？补充证明</h4><p>真的结束了吗。不妨思考，需要被采样的空间是一个有限空间吗？对目标分布进行采样，明明可以得到无数的采样值。这样之前证明的前提便全部被推翻了呀！<br>在此，引入新的概念，马尔科夫链的另一种分类，countable-state<br>chain，旨在描述类似连续分布采样过程的离散时间无限空间的马尔科夫链。那无限空间的马氏链仍然只有唯一的平稳分布吗。结论：对于不可约的马尔科夫链，如果$\\pi P=\\pi$有解，那么这个解是唯一的。相反，如果马氏链是正常反的，那么就一定有解。在此不对上述结论进行证明，有意者可以参考\\<Stochastic\nProcesses:Theory for Applications>的第298页的定理6.3.8.</p>\n"},{"title":"Tensor 储存字典的key，如何将字典的value填入Tensor","date":"2022-06-01T16:00:00.000Z","thumbnail":"https://tva3.sinaimg.cn/mw690/94aee95bgy1h2tsl9v8srj21hc0u0k3u.jpg","mathjax":true,"_content":"最近遇到一个问题，假设一个Tensor的元素都是标签（字典的key），这里我们还有一个对应的字典，如果想把key对应的value填回tensor，怎么操作呢？\n当然如果是在numpy中，我们可以循环遍历这个字典，然后修改元素的值，但是在tensor中是没有这么容易可以修改特定的元素的数值的。可能有些同学会自然想到把tensor转换成numpy.array再进行操作即可。这种想法不错，但是在使用keras 或者tensorflow时，在这里我默认大家在进行机器学习。显然循环操作和转换会大大减小效率。\ntf和keras中的gather函数可以解决这个问题，这里我以keras为例。\n***gather函数的官方解答***\n`keras.backend.gather(reference, indices)`\n\n```\n在张量 reference 中检索索引 indices 的元素。\n参数\nreference: 一个张量。\nindices: 索引的整数张量。\n返回 ：与 reference 类型相同的张量。\nNumpy 实现\ndef gather(reference, indices):\n    return reference[indices]\n```\n说白了就是在我们需要实现的功能，只不过这里的字典的key是一些索引，所以仅限数字，而且类型还的是int64\n\n\n__上实战代码：__\n\n```\nimport keras.backend as K\n\na = [[[1],[2],[2]],[[1],[1],[2]]]\nvariable = K.constant(a,dtype = 'int32')\nd1,d2,d3 = [1,2,3,1],[2,3,4,2],[3,4,5,5]\nc = {0:d1,1:d2,2:d3}\n#获取dict的所有value\nvalue = K.constant(list(c.values()))\n```\n==我们来看看value的值==\n\n```\nK.eval(value)\nOut[3]: \narray([[1., 2., 3., 1.],\n       [2., 3., 4., 2.],\n       [3., 4., 5., 5.]], dtype=float32)\n属性：Tensor(\"Const_1:0\", shape=(3, 4), dtype=float32)\n```\n接着操作   `K.gather(value,variable)`\n\n```\nOut[6]: \narray([[[[2., 3., 4., 2.]],\n        [[3., 4., 5., 5.]],\n        [[3., 4., 5., 5.]]],\n       [[[2., 3., 4., 2.]],\n        [[2., 3., 4., 2.]],\n        [[3., 4., 5., 5.]]]], dtype=float32)\nTensor(\"embedding_lookup_1/Identity:0\", shape=(2, 3, 1, 4), dtype=float32)\n```\n塔哒成功了，帮助到了你就给我评论下吧\n","source":"_posts/Tensor 储存字典的key，如何将字典的value填入Tensor.md","raw":"---\ntitle: Tensor 储存字典的key，如何将字典的value填入Tensor\ndate: 2022-06-02\ntags: tensorflow\ncategories: 编程\nthumbnail: https://tva3.sinaimg.cn/mw690/94aee95bgy1h2tsl9v8srj21hc0u0k3u.jpg\nmathjax: true\n---\n最近遇到一个问题，假设一个Tensor的元素都是标签（字典的key），这里我们还有一个对应的字典，如果想把key对应的value填回tensor，怎么操作呢？\n当然如果是在numpy中，我们可以循环遍历这个字典，然后修改元素的值，但是在tensor中是没有这么容易可以修改特定的元素的数值的。可能有些同学会自然想到把tensor转换成numpy.array再进行操作即可。这种想法不错，但是在使用keras 或者tensorflow时，在这里我默认大家在进行机器学习。显然循环操作和转换会大大减小效率。\ntf和keras中的gather函数可以解决这个问题，这里我以keras为例。\n***gather函数的官方解答***\n`keras.backend.gather(reference, indices)`\n\n```\n在张量 reference 中检索索引 indices 的元素。\n参数\nreference: 一个张量。\nindices: 索引的整数张量。\n返回 ：与 reference 类型相同的张量。\nNumpy 实现\ndef gather(reference, indices):\n    return reference[indices]\n```\n说白了就是在我们需要实现的功能，只不过这里的字典的key是一些索引，所以仅限数字，而且类型还的是int64\n\n\n__上实战代码：__\n\n```\nimport keras.backend as K\n\na = [[[1],[2],[2]],[[1],[1],[2]]]\nvariable = K.constant(a,dtype = 'int32')\nd1,d2,d3 = [1,2,3,1],[2,3,4,2],[3,4,5,5]\nc = {0:d1,1:d2,2:d3}\n#获取dict的所有value\nvalue = K.constant(list(c.values()))\n```\n==我们来看看value的值==\n\n```\nK.eval(value)\nOut[3]: \narray([[1., 2., 3., 1.],\n       [2., 3., 4., 2.],\n       [3., 4., 5., 5.]], dtype=float32)\n属性：Tensor(\"Const_1:0\", shape=(3, 4), dtype=float32)\n```\n接着操作   `K.gather(value,variable)`\n\n```\nOut[6]: \narray([[[[2., 3., 4., 2.]],\n        [[3., 4., 5., 5.]],\n        [[3., 4., 5., 5.]]],\n       [[[2., 3., 4., 2.]],\n        [[2., 3., 4., 2.]],\n        [[3., 4., 5., 5.]]]], dtype=float32)\nTensor(\"embedding_lookup_1/Identity:0\", shape=(2, 3, 1, 4), dtype=float32)\n```\n塔哒成功了，帮助到了你就给我评论下吧\n","slug":"Tensor 储存字典的key，如何将字典的value填入Tensor","published":1,"updated":"2022-06-02T04:36:08.491Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdp0001gsw12ndf4s4v","content":"<p>最近遇到一个问题，假设一个Tensor的元素都是标签（字典的key），这里我们还有一个对应的字典，如果想把key对应的value填回tensor，怎么操作呢？<br>当然如果是在numpy中，我们可以循环遍历这个字典，然后修改元素的值，但是在tensor中是没有这么容易可以修改特定的元素的数值的。可能有些同学会自然想到把tensor转换成numpy.array再进行操作即可。这种想法不错，但是在使用keras 或者tensorflow时，在这里我默认大家在进行机器学习。显然循环操作和转换会大大减小效率。<br>tf和keras中的gather函数可以解决这个问题，这里我以keras为例。<br><strong><em>gather函数的官方解答</em></strong><br><code>keras.backend.gather(reference, indices)</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在张量 reference 中检索索引 indices 的元素。</span><br><span class=\"line\">参数</span><br><span class=\"line\">reference: 一个张量。</span><br><span class=\"line\">indices: 索引的整数张量。</span><br><span class=\"line\">返回 ：与 reference 类型相同的张量。</span><br><span class=\"line\">Numpy 实现</span><br><span class=\"line\">def gather(reference, indices):</span><br><span class=\"line\">    return reference[indices]</span><br></pre></td></tr></table></figure>\n<p>说白了就是在我们需要实现的功能，只不过这里的字典的key是一些索引，所以仅限数字，而且类型还的是int64</p>\n<p><strong>上实战代码：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import keras.backend as K</span><br><span class=\"line\"></span><br><span class=\"line\">a = [[[1],[2],[2]],[[1],[1],[2]]]</span><br><span class=\"line\">variable = K.constant(a,dtype = &#x27;int32&#x27;)</span><br><span class=\"line\">d1,d2,d3 = [1,2,3,1],[2,3,4,2],[3,4,5,5]</span><br><span class=\"line\">c = &#123;0:d1,1:d2,2:d3&#125;</span><br><span class=\"line\">#获取dict的所有value</span><br><span class=\"line\">value = K.constant(list(c.values()))</span><br></pre></td></tr></table></figure>\n<p>==我们来看看value的值==</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">K.eval(value)</span><br><span class=\"line\">Out[3]: </span><br><span class=\"line\">array([[1., 2., 3., 1.],</span><br><span class=\"line\">       [2., 3., 4., 2.],</span><br><span class=\"line\">       [3., 4., 5., 5.]], dtype=float32)</span><br><span class=\"line\">属性：Tensor(&quot;Const_1:0&quot;, shape=(3, 4), dtype=float32)</span><br></pre></td></tr></table></figure>\n<p>接着操作   <code>K.gather(value,variable)</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Out[6]: </span><br><span class=\"line\">array([[[[2., 3., 4., 2.]],</span><br><span class=\"line\">        [[3., 4., 5., 5.]],</span><br><span class=\"line\">        [[3., 4., 5., 5.]]],</span><br><span class=\"line\">       [[[2., 3., 4., 2.]],</span><br><span class=\"line\">        [[2., 3., 4., 2.]],</span><br><span class=\"line\">        [[3., 4., 5., 5.]]]], dtype=float32)</span><br><span class=\"line\">Tensor(&quot;embedding_lookup_1/Identity:0&quot;, shape=(2, 3, 1, 4), dtype=float32)</span><br></pre></td></tr></table></figure>\n<p>塔哒成功了，帮助到了你就给我评论下吧</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近遇到一个问题，假设一个Tensor的元素都是标签（字典的key），这里我们还有一个对应的字典，如果想把key对应的value填回tensor，怎么操作呢？<br>当然如果是在numpy中，我们可以循环遍历这个字典，然后修改元素的值，但是在tensor中是没有这么容易可以修改特定的元素的数值的。可能有些同学会自然想到把tensor转换成numpy.array再进行操作即可。这种想法不错，但是在使用keras 或者tensorflow时，在这里我默认大家在进行机器学习。显然循环操作和转换会大大减小效率。<br>tf和keras中的gather函数可以解决这个问题，这里我以keras为例。<br><strong><em>gather函数的官方解答</em></strong><br><code>keras.backend.gather(reference, indices)</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在张量 reference 中检索索引 indices 的元素。</span><br><span class=\"line\">参数</span><br><span class=\"line\">reference: 一个张量。</span><br><span class=\"line\">indices: 索引的整数张量。</span><br><span class=\"line\">返回 ：与 reference 类型相同的张量。</span><br><span class=\"line\">Numpy 实现</span><br><span class=\"line\">def gather(reference, indices):</span><br><span class=\"line\">    return reference[indices]</span><br></pre></td></tr></table></figure>\n<p>说白了就是在我们需要实现的功能，只不过这里的字典的key是一些索引，所以仅限数字，而且类型还的是int64</p>\n<p><strong>上实战代码：</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import keras.backend as K</span><br><span class=\"line\"></span><br><span class=\"line\">a = [[[1],[2],[2]],[[1],[1],[2]]]</span><br><span class=\"line\">variable = K.constant(a,dtype = &#x27;int32&#x27;)</span><br><span class=\"line\">d1,d2,d3 = [1,2,3,1],[2,3,4,2],[3,4,5,5]</span><br><span class=\"line\">c = &#123;0:d1,1:d2,2:d3&#125;</span><br><span class=\"line\">#获取dict的所有value</span><br><span class=\"line\">value = K.constant(list(c.values()))</span><br></pre></td></tr></table></figure>\n<p>==我们来看看value的值==</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">K.eval(value)</span><br><span class=\"line\">Out[3]: </span><br><span class=\"line\">array([[1., 2., 3., 1.],</span><br><span class=\"line\">       [2., 3., 4., 2.],</span><br><span class=\"line\">       [3., 4., 5., 5.]], dtype=float32)</span><br><span class=\"line\">属性：Tensor(&quot;Const_1:0&quot;, shape=(3, 4), dtype=float32)</span><br></pre></td></tr></table></figure>\n<p>接着操作   <code>K.gather(value,variable)</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Out[6]: </span><br><span class=\"line\">array([[[[2., 3., 4., 2.]],</span><br><span class=\"line\">        [[3., 4., 5., 5.]],</span><br><span class=\"line\">        [[3., 4., 5., 5.]]],</span><br><span class=\"line\">       [[[2., 3., 4., 2.]],</span><br><span class=\"line\">        [[2., 3., 4., 2.]],</span><br><span class=\"line\">        [[3., 4., 5., 5.]]]], dtype=float32)</span><br><span class=\"line\">Tensor(&quot;embedding_lookup_1/Identity:0&quot;, shape=(2, 3, 1, 4), dtype=float32)</span><br></pre></td></tr></table></figure>\n<p>塔哒成功了，帮助到了你就给我评论下吧</p>\n"},{"title":"Trace—σ代数的证明","date":"2022-06-01T16:00:00.000Z","thumbnail":"https://tva3.sinaimg.cn/mw690/94aee95bgy1h2tsk2u7lej22bc1jk7nx.jpg","mathjax":true,"_content":"本文旨在证明 Trace $\\sigma$ algebra 是一个algebra。Trace $\\sigma$ algebra 的定义是：\n\n> 假设$E\\subseteq  X$，且$\\mathscr{A}$是X上的$\\sigma$代数，那么\n$\\mathscr{A}_{E}$ $\\vcentcolon=$ $\\{E \\cap{A} , A\\in\\mathscr{A}\\}$是一个$E$上的$\\sigma$ algebra。\n\n>证明，根据定义的三条，第一条$E\\in\\mathscr{A}_{E}$明显满足。\n>第二条，需要证明$E$的子集$A\\in\\mathscr{A}_{E}$那么$E\\setminus A\\in\\mathscr{A}_{E}$\n>假设一些S=E$\\cap$A$,$A$\\in$$\\mathscr{A}_E$，因此，$X\\setminus S\\in\\mathscr{A}$,那么根据定义$E\\cap(X\\setminus S)\\in\\mathscr{A}_E$而$E\\cap(X\\setminus S)=(E\\cap X)\\setminus S=E\\setminus S$,所以$E\\setminus S \\in \\mathscr{A}_E$。\n\n第三条需要证明的条件是：$A_i\\in \\mathscr{A}_E,那么\\cup _{i\\in\\mathbb{N}}A_i\\in\\mathscr{A}_E$。\n证明过程如下：\n根据定义假设$S_i=E\\cap A_i$其中$A_i \\in \\mathscr{A}$,那么$\\cup S_i=E\\cap \\{\\cup A_i\\}$，由于$\\mathscr{A}$是$\\sigma$-algebra，所以$\\{\\cup A_i\\} \\in \\mathscr{A}$ ,根据定义$\\cup S_i \\in \\mathscr{A}_E$.","source":"_posts/Trace—σ代数的证明.md","raw":"---\ntitle: Trace—σ代数的证明\ndate: 2022-06-02\ntags: 统计学\ncategories: 信号\nthumbnail: https://tva3.sinaimg.cn/mw690/94aee95bgy1h2tsk2u7lej22bc1jk7nx.jpg\nmathjax: true\n---\n本文旨在证明 Trace $\\sigma$ algebra 是一个algebra。Trace $\\sigma$ algebra 的定义是：\n\n> 假设$E\\subseteq  X$，且$\\mathscr{A}$是X上的$\\sigma$代数，那么\n$\\mathscr{A}_{E}$ $\\vcentcolon=$ $\\{E \\cap{A} , A\\in\\mathscr{A}\\}$是一个$E$上的$\\sigma$ algebra。\n\n>证明，根据定义的三条，第一条$E\\in\\mathscr{A}_{E}$明显满足。\n>第二条，需要证明$E$的子集$A\\in\\mathscr{A}_{E}$那么$E\\setminus A\\in\\mathscr{A}_{E}$\n>假设一些S=E$\\cap$A$,$A$\\in$$\\mathscr{A}_E$，因此，$X\\setminus S\\in\\mathscr{A}$,那么根据定义$E\\cap(X\\setminus S)\\in\\mathscr{A}_E$而$E\\cap(X\\setminus S)=(E\\cap X)\\setminus S=E\\setminus S$,所以$E\\setminus S \\in \\mathscr{A}_E$。\n\n第三条需要证明的条件是：$A_i\\in \\mathscr{A}_E,那么\\cup _{i\\in\\mathbb{N}}A_i\\in\\mathscr{A}_E$。\n证明过程如下：\n根据定义假设$S_i=E\\cap A_i$其中$A_i \\in \\mathscr{A}$,那么$\\cup S_i=E\\cap \\{\\cup A_i\\}$，由于$\\mathscr{A}$是$\\sigma$-algebra，所以$\\{\\cup A_i\\} \\in \\mathscr{A}$ ,根据定义$\\cup S_i \\in \\mathscr{A}_E$.","slug":"Trace—σ代数的证明","published":1,"updated":"2022-06-02T04:34:51.540Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdt0004gsw18txz4v6t","content":"<p>本文旨在证明 Trace $\\sigma$ algebra 是一个algebra。Trace $\\sigma$ algebra 的定义是：</p>\n<blockquote>\n<p>假设$E\\subseteq  X$，且$\\mathscr{A}$是X上的$\\sigma$代数，那么<br>$\\mathscr{A}_{E}$ $\\vcentcolon=$ $\\{E \\cap{A} , A\\in\\mathscr{A}\\}$是一个$E$上的$\\sigma$ algebra。</p>\n<p>证明，根据定义的三条，第一条$E\\in\\mathscr{A}_{E}$明显满足。<br>第二条，需要证明$E$的子集$A\\in\\mathscr{A}_{E}$那么$E\\setminus A\\in\\mathscr{A}_{E}$<br>假设一些S=E$\\cap$A$,$A$\\in$$\\mathscr{A}_E$，因此，$X\\setminus S\\in\\mathscr{A}$,那么根据定义$E\\cap(X\\setminus S)\\in\\mathscr{A}_E$而$E\\cap(X\\setminus S)=(E\\cap X)\\setminus S=E\\setminus S$,所以$E\\setminus S \\in \\mathscr{A}_E$。</p>\n</blockquote>\n<p>第三条需要证明的条件是：$A_i\\in \\mathscr{A}_E,那么\\cup _{i\\in\\mathbb{N}}A_i\\in\\mathscr{A}_E$。<br>证明过程如下：<br>根据定义假设$S_i=E\\cap A_i$其中$A_i \\in \\mathscr{A}$,那么$\\cup S_i=E\\cap \\{\\cup A_i\\}$，由于$\\mathscr{A}$是$\\sigma$-algebra，所以$\\{\\cup A_i\\} \\in \\mathscr{A}$ ,根据定义$\\cup S_i \\in \\mathscr{A}_E$.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文旨在证明 Trace $\\sigma$ algebra 是一个algebra。Trace $\\sigma$ algebra 的定义是：</p>\n<blockquote>\n<p>假设$E\\subseteq  X$，且$\\mathscr{A}$是X上的$\\sigma$代数，那么<br>$\\mathscr{A}_{E}$ $\\vcentcolon=$ $\\{E \\cap{A} , A\\in\\mathscr{A}\\}$是一个$E$上的$\\sigma$ algebra。</p>\n<p>证明，根据定义的三条，第一条$E\\in\\mathscr{A}_{E}$明显满足。<br>第二条，需要证明$E$的子集$A\\in\\mathscr{A}_{E}$那么$E\\setminus A\\in\\mathscr{A}_{E}$<br>假设一些S=E$\\cap$A$,$A$\\in$$\\mathscr{A}_E$，因此，$X\\setminus S\\in\\mathscr{A}$,那么根据定义$E\\cap(X\\setminus S)\\in\\mathscr{A}_E$而$E\\cap(X\\setminus S)=(E\\cap X)\\setminus S=E\\setminus S$,所以$E\\setminus S \\in \\mathscr{A}_E$。</p>\n</blockquote>\n<p>第三条需要证明的条件是：$A_i\\in \\mathscr{A}_E,那么\\cup _{i\\in\\mathbb{N}}A_i\\in\\mathscr{A}_E$。<br>证明过程如下：<br>根据定义假设$S_i=E\\cap A_i$其中$A_i \\in \\mathscr{A}$,那么$\\cup S_i=E\\cap \\{\\cup A_i\\}$，由于$\\mathscr{A}$是$\\sigma$-algebra，所以$\\{\\cup A_i\\} \\in \\mathscr{A}$ ,根据定义$\\cup S_i \\in \\mathscr{A}_E$.</p>\n"},{"title":"复随机变量","date":"2022-06-01T16:00:00.000Z","thumbnail":"https://tvax3.sinaimg.cn/large/94aee95bgy1h2tsirg24xj22yo1o0gti.jpg","mathjax":true,"_content":"## Proper Complex Random Vectors\n**定义：**\n&emsp;当一个复随机矢量$\\mathbf{Z}$满足以下条件时，我们称它为**Proper**:\n\n 1. 均值为0\n 2. 方差为有限值(不是$\\pm\\infty$)\n 3. $\\mathrm{E}[\\mathbf{ZZ}^T]=0$\n\n","source":"_posts/复随机变量.md","raw":"---\ntitle: 复随机变量\ndate: 2022-06-02\ntags: 统计学\ncategories: 信号\nthumbnail: https://tvax3.sinaimg.cn/large/94aee95bgy1h2tsirg24xj22yo1o0gti.jpg\nmathjax: true\n---\n## Proper Complex Random Vectors\n**定义：**\n&emsp;当一个复随机矢量$\\mathbf{Z}$满足以下条件时，我们称它为**Proper**:\n\n 1. 均值为0\n 2. 方差为有限值(不是$\\pm\\infty$)\n 3. $\\mathrm{E}[\\mathbf{ZZ}^T]=0$\n\n","slug":"复随机变量","published":1,"updated":"2022-06-02T04:33:37.072Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdu0005gsw1a0fo1stj","content":"<h2 id=\"Proper-Complex-Random-Vectors\"><a href=\"#Proper-Complex-Random-Vectors\" class=\"headerlink\" title=\"Proper Complex Random Vectors\"></a>Proper Complex Random Vectors</h2><p><strong>定义：</strong><br>&emsp;当一个复随机矢量$\\mathbf{Z}$满足以下条件时，我们称它为<strong>Proper</strong>:</p>\n<ol>\n<li>均值为0</li>\n<li>方差为有限值(不是$\\pm\\infty$)</li>\n<li>$\\mathrm{E}[\\mathbf{ZZ}^T]=0$</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Proper-Complex-Random-Vectors\"><a href=\"#Proper-Complex-Random-Vectors\" class=\"headerlink\" title=\"Proper Complex Random Vectors\"></a>Proper Complex Random Vectors</h2><p><strong>定义：</strong><br>&emsp;当一个复随机矢量$\\mathbf{Z}$满足以下条件时，我们称它为<strong>Proper</strong>:</p>\n<ol>\n<li>均值为0</li>\n<li>方差为有限值(不是$\\pm\\infty$)</li>\n<li>$\\mathrm{E}[\\mathbf{ZZ}^T]=0$</li>\n</ol>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2022-06-01T11:08:45.809Z","updated":"2022-06-01T11:08:45.809Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdv0006gsw1cg5te80q","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"官方文档没讲清楚？？？Pytorch中神奇函数方法map()","date":"2022-06-01T16:00:00.000Z","thumbnail":"https://tvax3.sinaimg.cn/mw690/94aee95bgy1h2tsmk5ivwj24mo334qv6.jpg","mathjax":true,"_content":"\n在python中有一个高级函数叫做map()\n引用廖雪峰的讲解\n\n> 如果你读过Google的那篇大名鼎鼎的论文“MapReduce: Simplified Data Processing on Large Clusters”，你就能大概明白map/reduce的概念。\n> 我们先看map。map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。\n> 举例说明，比如我们有一个函数$f(x)=x^2$，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下：\n\n![$f(x)=x2$](https://img-blog.csdnimg.cn/20200721170555544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n代码实现如下：\n```\ndef f(x):\n\t return x * x\n r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])\n list(r)\n [1, 4, 9, 16, 25, 36, 49, 64, 81]\n```\n说的很清楚了，返回的是一个$Iterator$,那么在pytorch中相对torch.Tensor中的每一个element进行函数运算有没有torch的函数呢，我查了下，还真有。来看看官方文档的教程\n\n```\nmap_(tensor, callable)\nApplies callable for each element in self tensor and the given tensor and stores the results in self tensor. \nself tensor and the given tensor must be broadcastable.\n\nThe callable should have the signature:\ndef callable(a, b) -> number\n```\n等等？这是神马？完全没说清楚啊，$callable()$我可以理解，就是可调用的对象。但是这个参数$(a,b)$是哪里来的？？？？$python$中的$map$不是$f$接受$Iteratorable$里面返回的每一个对象吗，返回一个就是一个参数，返回两个就是两个参数啊，这里为啥是$(a,b)$两个参数？？？？\n假设有一个tensor,我想令每一个元素的操作编程$f = 4\\times{x}+10$\n\n```python\nimprot torch\ndef f(x): return 4*x+10\ntorch.randint(0,3,(2,3))\nprint(x)\n```\n\n```python\ntensor([[0, 0, 0],\n        [0, 1, 2]])\n```\n这个时候对$x$用上$map\\_()$函数\n\n```python\nx.map_(x,f)\n\n出现的是这个：TypeError: f() takes 1 positional argument but 2 were given\n```\n咦，怎么还真是传入了两个参数啊，这两个参数是啥啊？？\n**好了这里就不卖关子了** ，这两个参数其实就是$Iteratorable$里面返回的每一个对象的两份同样的值，也就是说$a=b$,那我们需要把函数改一下,传入两个参数，但是只要一个就行了，或者使用多参数的形式$*karg$：\n\n```python\nimport torch\ndef f(x,*y): return 4*x+10\nx = torch.randint(0,3,(2,3))\nprint('原来的x是\\n{}'.format(x))\nx.map_(x,f)\nprint(x)\n```\n\n```python\n原来的x是\ntensor([[1, 0, 0],\n        [2, 2, 2]])\ntensor([[14, 10, 10],\n        [18, 18, 18]])\n```\n这样就完成了我们的目标，代码也变得简洁了很多，是不是很开心呢？\n\n**原创不易，点个赞再走吧！**\n\n","source":"_posts/官方文档没讲清楚？？？Pytorch中神奇函数方法map()_.md","raw":"---\ntitle: 官方文档没讲清楚？？？Pytorch中神奇函数方法map()\ndate: 2022-06-02\ntags: Pytorch\ncategories: 编程\nthumbnail: https://tvax3.sinaimg.cn/mw690/94aee95bgy1h2tsmk5ivwj24mo334qv6.jpg\nmathjax: true\n---\n\n在python中有一个高级函数叫做map()\n引用廖雪峰的讲解\n\n> 如果你读过Google的那篇大名鼎鼎的论文“MapReduce: Simplified Data Processing on Large Clusters”，你就能大概明白map/reduce的概念。\n> 我们先看map。map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。\n> 举例说明，比如我们有一个函数$f(x)=x^2$，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下：\n\n![$f(x)=x2$](https://img-blog.csdnimg.cn/20200721170555544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n代码实现如下：\n```\ndef f(x):\n\t return x * x\n r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])\n list(r)\n [1, 4, 9, 16, 25, 36, 49, 64, 81]\n```\n说的很清楚了，返回的是一个$Iterator$,那么在pytorch中相对torch.Tensor中的每一个element进行函数运算有没有torch的函数呢，我查了下，还真有。来看看官方文档的教程\n\n```\nmap_(tensor, callable)\nApplies callable for each element in self tensor and the given tensor and stores the results in self tensor. \nself tensor and the given tensor must be broadcastable.\n\nThe callable should have the signature:\ndef callable(a, b) -> number\n```\n等等？这是神马？完全没说清楚啊，$callable()$我可以理解，就是可调用的对象。但是这个参数$(a,b)$是哪里来的？？？？$python$中的$map$不是$f$接受$Iteratorable$里面返回的每一个对象吗，返回一个就是一个参数，返回两个就是两个参数啊，这里为啥是$(a,b)$两个参数？？？？\n假设有一个tensor,我想令每一个元素的操作编程$f = 4\\times{x}+10$\n\n```python\nimprot torch\ndef f(x): return 4*x+10\ntorch.randint(0,3,(2,3))\nprint(x)\n```\n\n```python\ntensor([[0, 0, 0],\n        [0, 1, 2]])\n```\n这个时候对$x$用上$map\\_()$函数\n\n```python\nx.map_(x,f)\n\n出现的是这个：TypeError: f() takes 1 positional argument but 2 were given\n```\n咦，怎么还真是传入了两个参数啊，这两个参数是啥啊？？\n**好了这里就不卖关子了** ，这两个参数其实就是$Iteratorable$里面返回的每一个对象的两份同样的值，也就是说$a=b$,那我们需要把函数改一下,传入两个参数，但是只要一个就行了，或者使用多参数的形式$*karg$：\n\n```python\nimport torch\ndef f(x,*y): return 4*x+10\nx = torch.randint(0,3,(2,3))\nprint('原来的x是\\n{}'.format(x))\nx.map_(x,f)\nprint(x)\n```\n\n```python\n原来的x是\ntensor([[1, 0, 0],\n        [2, 2, 2]])\ntensor([[14, 10, 10],\n        [18, 18, 18]])\n```\n这样就完成了我们的目标，代码也变得简洁了很多，是不是很开心呢？\n\n**原创不易，点个赞再走吧！**\n\n","slug":"官方文档没讲清楚？？？Pytorch中神奇函数方法map()_","published":1,"updated":"2022-06-02T04:37:17.314Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdx000agsw1flmdbc0l","content":"<p>在python中有一个高级函数叫做map()<br>引用廖雪峰的讲解</p>\n<blockquote>\n<p>如果你读过Google的那篇大名鼎鼎的论文“MapReduce: Simplified Data Processing on Large Clusters”，你就能大概明白map/reduce的概念。<br>我们先看map。map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。<br>举例说明，比如我们有一个函数$f(x)=x^2$，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下：</p>\n</blockquote>\n<p><img src=\"https://img-blog.csdnimg.cn/20200721170555544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"$f(x)=x2$\"><br>代码实现如下：<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def f(x):</span><br><span class=\"line\">\t return x * x</span><br><span class=\"line\"> r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br><span class=\"line\"> list(r)</span><br><span class=\"line\"> [1, 4, 9, 16, 25, 36, 49, 64, 81]</span><br></pre></td></tr></table></figure><br>说的很清楚了，返回的是一个$Iterator$,那么在pytorch中相对torch.Tensor中的每一个element进行函数运算有没有torch的函数呢，我查了下，还真有。来看看官方文档的教程</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">map_(tensor, callable)</span><br><span class=\"line\">Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. </span><br><span class=\"line\">self tensor and the given tensor must be broadcastable.</span><br><span class=\"line\"></span><br><span class=\"line\">The callable should have the signature:</span><br><span class=\"line\">def callable(a, b) -&gt; number</span><br></pre></td></tr></table></figure>\n<p>等等？这是神马？完全没说清楚啊，$callable()$我可以理解，就是可调用的对象。但是这个参数$(a,b)$是哪里来的？？？？$python$中的$map$不是$f$接受$Iteratorable$里面返回的每一个对象吗，返回一个就是一个参数，返回两个就是两个参数啊，这里为啥是$(a,b)$两个参数？？？？<br>假设有一个tensor,我想令每一个元素的操作编程$f = 4\\times{x}+10$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">improt torch</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>): <span class=\"keyword\">return</span> <span class=\"number\">4</span>*x+<span class=\"number\">10</span></span><br><span class=\"line\">torch.randint(<span class=\"number\">0</span>,<span class=\"number\">3</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>]])</span><br></pre></td></tr></table></figure>\n<p>这个时候对$x$用上$map_()$函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.map_(x,f)</span><br><span class=\"line\"></span><br><span class=\"line\">出现的是这个：TypeError: f() takes <span class=\"number\">1</span> positional argument but <span class=\"number\">2</span> were given</span><br></pre></td></tr></table></figure>\n<p>咦，怎么还真是传入了两个参数啊，这两个参数是啥啊？？<br><strong>好了这里就不卖关子了</strong> ，这两个参数其实就是$Iteratorable$里面返回的每一个对象的两份同样的值，也就是说$a=b$,那我们需要把函数改一下,传入两个参数，但是只要一个就行了，或者使用多参数的形式$*karg$：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x,*y</span>): <span class=\"keyword\">return</span> <span class=\"number\">4</span>*x+<span class=\"number\">10</span></span><br><span class=\"line\">x = torch.randint(<span class=\"number\">0</span>,<span class=\"number\">3</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;原来的x是\\n&#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(x))</span><br><span class=\"line\">x.map_(x,f)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">原来的x是</span><br><span class=\"line\">tensor([[<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]])</span><br><span class=\"line\">tensor([[<span class=\"number\">14</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>],</span><br><span class=\"line\">        [<span class=\"number\">18</span>, <span class=\"number\">18</span>, <span class=\"number\">18</span>]])</span><br></pre></td></tr></table></figure>\n<p>这样就完成了我们的目标，代码也变得简洁了很多，是不是很开心呢？</p>\n<p><strong>原创不易，点个赞再走吧！</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>在python中有一个高级函数叫做map()<br>引用廖雪峰的讲解</p>\n<blockquote>\n<p>如果你读过Google的那篇大名鼎鼎的论文“MapReduce: Simplified Data Processing on Large Clusters”，你就能大概明白map/reduce的概念。<br>我们先看map。map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。<br>举例说明，比如我们有一个函数$f(x)=x^2$，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下：</p>\n</blockquote>\n<p><img src=\"https://img-blog.csdnimg.cn/20200721170555544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"$f(x)=x2$\"><br>代码实现如下：<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def f(x):</span><br><span class=\"line\">\t return x * x</span><br><span class=\"line\"> r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br><span class=\"line\"> list(r)</span><br><span class=\"line\"> [1, 4, 9, 16, 25, 36, 49, 64, 81]</span><br></pre></td></tr></table></figure><br>说的很清楚了，返回的是一个$Iterator$,那么在pytorch中相对torch.Tensor中的每一个element进行函数运算有没有torch的函数呢，我查了下，还真有。来看看官方文档的教程</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">map_(tensor, callable)</span><br><span class=\"line\">Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. </span><br><span class=\"line\">self tensor and the given tensor must be broadcastable.</span><br><span class=\"line\"></span><br><span class=\"line\">The callable should have the signature:</span><br><span class=\"line\">def callable(a, b) -&gt; number</span><br></pre></td></tr></table></figure>\n<p>等等？这是神马？完全没说清楚啊，$callable()$我可以理解，就是可调用的对象。但是这个参数$(a,b)$是哪里来的？？？？$python$中的$map$不是$f$接受$Iteratorable$里面返回的每一个对象吗，返回一个就是一个参数，返回两个就是两个参数啊，这里为啥是$(a,b)$两个参数？？？？<br>假设有一个tensor,我想令每一个元素的操作编程$f = 4\\times{x}+10$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">improt torch</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>): <span class=\"keyword\">return</span> <span class=\"number\">4</span>*x+<span class=\"number\">10</span></span><br><span class=\"line\">torch.randint(<span class=\"number\">0</span>,<span class=\"number\">3</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>]])</span><br></pre></td></tr></table></figure>\n<p>这个时候对$x$用上$map_()$函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.map_(x,f)</span><br><span class=\"line\"></span><br><span class=\"line\">出现的是这个：TypeError: f() takes <span class=\"number\">1</span> positional argument but <span class=\"number\">2</span> were given</span><br></pre></td></tr></table></figure>\n<p>咦，怎么还真是传入了两个参数啊，这两个参数是啥啊？？<br><strong>好了这里就不卖关子了</strong> ，这两个参数其实就是$Iteratorable$里面返回的每一个对象的两份同样的值，也就是说$a=b$,那我们需要把函数改一下,传入两个参数，但是只要一个就行了，或者使用多参数的形式$*karg$：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x,*y</span>): <span class=\"keyword\">return</span> <span class=\"number\">4</span>*x+<span class=\"number\">10</span></span><br><span class=\"line\">x = torch.randint(<span class=\"number\">0</span>,<span class=\"number\">3</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;原来的x是\\n&#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(x))</span><br><span class=\"line\">x.map_(x,f)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">原来的x是</span><br><span class=\"line\">tensor([[<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>]])</span><br><span class=\"line\">tensor([[<span class=\"number\">14</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>],</span><br><span class=\"line\">        [<span class=\"number\">18</span>, <span class=\"number\">18</span>, <span class=\"number\">18</span>]])</span><br></pre></td></tr></table></figure>\n<p>这样就完成了我们的目标，代码也变得简洁了很多，是不是很开心呢？</p>\n<p><strong>原创不易，点个赞再走吧！</strong></p>\n"},{"title":"正态分布推导瑞利分布，瑞利信道的模型","date":"2022-06-01T13:39:59.000Z","thumbnail":"https://tva2.sinaimg.cn/large/94aee95bgy1h2t32urys8j21z414014l.jpg","mathjax":true,"_content":"## 从高斯分布推导瑞利分布\n瑞利分布是无线通信中常见的信道模型，这里就来推导一下，所谓瑞利分布就是两个垂直分量服从独立且相同的标准高斯分布叠加之后的模。先来看看高斯分布的表达式\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2{\\sigma}^2}\\right)$$  \n其中$\\sigma$是分布的方差，$\\mu$是分布的均值。\n假设$X_1,X_2\\sim N(0,{\\sigma}^2)$,$X^2 = {X_1}^2+{X_2}^2$,现在需要推导$X$的概率密度函数。$x_1$和$x_2$的联合概率密度 如下：\n$$\\begin{aligned}\nf(x_1,x_2) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right).\n\\end{aligned} $$\n$$\\begin{aligned}\nF(X)=P(X\\leq x)&= P(\\sqrt{ {X_1}^2+{X_2}^2}\\leq x)\\\\\n&=\\iint\\limits_ { {x_1}^2+{x_2}^2{\\leq}x}\\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right)dx_1dx_2\\\\\n&=\\frac{1}{2\\pi\\sigma^2}\\int_{0}^{2\\pi}d\\theta\\int_{0}^{x}r\\exp\\left(-\\frac{r^2}{2{\\sigma}^2}\\right)dr\\\\\n&=1-\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)\n\\end{aligned}$$\n此时$f(x)=F'(x)=\\frac{x}{ {\\sigma}^2}\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)$,注意上述积分的定义域是$x\\geq0$","source":"_posts/正态分布推导瑞利分布，瑞利信道的模型.md","raw":"---\ntitle: 正态分布推导瑞利分布，瑞利信道的模型\ndate: 2022-06-01 21:39:59\ntags:\ncategories: 信号\nthumbnail: https://tva2.sinaimg.cn/large/94aee95bgy1h2t32urys8j21z414014l.jpg\nmathjax: true\n---\n## 从高斯分布推导瑞利分布\n瑞利分布是无线通信中常见的信道模型，这里就来推导一下，所谓瑞利分布就是两个垂直分量服从独立且相同的标准高斯分布叠加之后的模。先来看看高斯分布的表达式\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2{\\sigma}^2}\\right)$$  \n其中$\\sigma$是分布的方差，$\\mu$是分布的均值。\n假设$X_1,X_2\\sim N(0,{\\sigma}^2)$,$X^2 = {X_1}^2+{X_2}^2$,现在需要推导$X$的概率密度函数。$x_1$和$x_2$的联合概率密度 如下：\n$$\\begin{aligned}\nf(x_1,x_2) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right).\n\\end{aligned} $$\n$$\\begin{aligned}\nF(X)=P(X\\leq x)&= P(\\sqrt{ {X_1}^2+{X_2}^2}\\leq x)\\\\\n&=\\iint\\limits_ { {x_1}^2+{x_2}^2{\\leq}x}\\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right)dx_1dx_2\\\\\n&=\\frac{1}{2\\pi\\sigma^2}\\int_{0}^{2\\pi}d\\theta\\int_{0}^{x}r\\exp\\left(-\\frac{r^2}{2{\\sigma}^2}\\right)dr\\\\\n&=1-\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)\n\\end{aligned}$$\n此时$f(x)=F'(x)=\\frac{x}{ {\\sigma}^2}\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)$,注意上述积分的定义域是$x\\geq0$","slug":"正态分布推导瑞利分布，瑞利信道的模型","published":1,"updated":"2022-06-01T14:32:15.833Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0gdy000cgsw19g4c8q2e","content":"<h2 id=\"从高斯分布推导瑞利分布\"><a href=\"#从高斯分布推导瑞利分布\" class=\"headerlink\" title=\"从高斯分布推导瑞利分布\"></a>从高斯分布推导瑞利分布</h2><p>瑞利分布是无线通信中常见的信道模型，这里就来推导一下，所谓瑞利分布就是两个垂直分量服从独立且相同的标准高斯分布叠加之后的模。先来看看高斯分布的表达式</p>\n<script type=\"math/tex; mode=display\">f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2{\\sigma}^2}\\right)</script><p>其中$\\sigma$是分布的方差，$\\mu$是分布的均值。<br>假设$X_1,X_2\\sim N(0,{\\sigma}^2)$,$X^2 = {X_1}^2+{X_2}^2$,现在需要推导$X$的概率密度函数。$x_1$和$x_2$的联合概率密度 如下：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\nf(x_1,x_2) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right).\n\\end{aligned}</script><script type=\"math/tex; mode=display\">\\begin{aligned}\nF(X)=P(X\\leq x)&= P(\\sqrt{ {X_1}^2+{X_2}^2}\\leq x)\\\\\n&=\\iint\\limits_ { {x_1}^2+{x_2}^2{\\leq}x}\\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right)dx_1dx_2\\\\\n&=\\frac{1}{2\\pi\\sigma^2}\\int_{0}^{2\\pi}d\\theta\\int_{0}^{x}r\\exp\\left(-\\frac{r^2}{2{\\sigma}^2}\\right)dr\\\\\n&=1-\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)\n\\end{aligned}</script><p>此时$f(x)=F’(x)=\\frac{x}{ {\\sigma}^2}\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)$,注意上述积分的定义域是$x\\geq0$</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"从高斯分布推导瑞利分布\"><a href=\"#从高斯分布推导瑞利分布\" class=\"headerlink\" title=\"从高斯分布推导瑞利分布\"></a>从高斯分布推导瑞利分布</h2><p>瑞利分布是无线通信中常见的信道模型，这里就来推导一下，所谓瑞利分布就是两个垂直分量服从独立且相同的标准高斯分布叠加之后的模。先来看看高斯分布的表达式</p>\n<script type=\"math/tex; mode=display\">f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2{\\sigma}^2}\\right)</script><p>其中$\\sigma$是分布的方差，$\\mu$是分布的均值。<br>假设$X_1,X_2\\sim N(0,{\\sigma}^2)$,$X^2 = {X_1}^2+{X_2}^2$,现在需要推导$X$的概率密度函数。$x_1$和$x_2$的联合概率密度 如下：</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\nf(x_1,x_2) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right).\n\\end{aligned}</script><script type=\"math/tex; mode=display\">\\begin{aligned}\nF(X)=P(X\\leq x)&= P(\\sqrt{ {X_1}^2+{X_2}^2}\\leq x)\\\\\n&=\\iint\\limits_ { {x_1}^2+{x_2}^2{\\leq}x}\\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x_1^2+x_2^2}{2{\\sigma}^2}\\right)dx_1dx_2\\\\\n&=\\frac{1}{2\\pi\\sigma^2}\\int_{0}^{2\\pi}d\\theta\\int_{0}^{x}r\\exp\\left(-\\frac{r^2}{2{\\sigma}^2}\\right)dr\\\\\n&=1-\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)\n\\end{aligned}</script><p>此时$f(x)=F’(x)=\\frac{x}{ {\\sigma}^2}\\exp\\left(-\\frac{x^2}{2{\\sigma}^2}\\right)$,注意上述积分的定义域是$x\\geq0$</p>\n"},{"title":"高斯分布的数乘，相加，相乘还是高斯分布吗","date":"2022-06-01T16:00:00.000Z","thumbnail":"https://tva1.sinaimg.cn/large/94aee95bgy1h2tshnxn94j21hc0u04qp.jpg","mathjax":true,"_content":"# 高斯分布的数乘，相加，相乘还是高斯分布吗？\n&emsp;首先当随机变量$Y=g(X)$时,两者的PDF具有这样的关系$$f_{Y}(y)=f_{X}(x)\\lvert\\frac{dx}{dy}\\rvert$$\n具体来说$$F_{Y}(y)=P(Y\\leq y)=P\\left(g(X)\\leq y\\right)=P\\left(X\\leq g^{-1}(y)\\right)=F_{X}( g^{-1}(y)) = F_{X}(x)$$\n知道上述关系之后，考虑随机变量$Y=aX$，其中$X\\sim\\mathcal{N}(\\mu,\\sigma^2)$，则\n$$\\begin{aligned}f_{Y}(y)&=\\frac{1}{a}f_{X}(x)\\\\\n&=\\frac{1}{a}f_{X}(\\frac{y}{a})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}a\\sigma}\\exp(-\\frac{(\\frac{y}{a}-\\mu)^2}{2\\sigma^2})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}(a\\sigma)}\\exp(-\\frac{(y-a\\mu)^2}{2(a\\sigma)^2})\n\\end{aligned}$$\n所以$Y\\sim\\mathcal{N}(a\\mu,(a\\sigma^2))$，可得知高斯分布的数乘还是高斯分布。\n","source":"_posts/高斯分布的数乘，相加，相乘还是高斯分布吗？.md","raw":"---\ntitle: 高斯分布的数乘，相加，相乘还是高斯分布吗\ndate: 2022-06-02\ntags:\ncategories: 信号\nthumbnail: https://tva1.sinaimg.cn/large/94aee95bgy1h2tshnxn94j21hc0u04qp.jpg\nmathjax: true\n---\n# 高斯分布的数乘，相加，相乘还是高斯分布吗？\n&emsp;首先当随机变量$Y=g(X)$时,两者的PDF具有这样的关系$$f_{Y}(y)=f_{X}(x)\\lvert\\frac{dx}{dy}\\rvert$$\n具体来说$$F_{Y}(y)=P(Y\\leq y)=P\\left(g(X)\\leq y\\right)=P\\left(X\\leq g^{-1}(y)\\right)=F_{X}( g^{-1}(y)) = F_{X}(x)$$\n知道上述关系之后，考虑随机变量$Y=aX$，其中$X\\sim\\mathcal{N}(\\mu,\\sigma^2)$，则\n$$\\begin{aligned}f_{Y}(y)&=\\frac{1}{a}f_{X}(x)\\\\\n&=\\frac{1}{a}f_{X}(\\frac{y}{a})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}a\\sigma}\\exp(-\\frac{(\\frac{y}{a}-\\mu)^2}{2\\sigma^2})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}(a\\sigma)}\\exp(-\\frac{(y-a\\mu)^2}{2(a\\sigma)^2})\n\\end{aligned}$$\n所以$Y\\sim\\mathcal{N}(a\\mu,(a\\sigma^2))$，可得知高斯分布的数乘还是高斯分布。\n","slug":"高斯分布的数乘，相加，相乘还是高斯分布吗？","published":1,"updated":"2022-06-02T04:32:36.891Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0ge1000ggsw12tvo6ar1","content":"<h1 id=\"高斯分布的数乘，相加，相乘还是高斯分布吗？\"><a href=\"#高斯分布的数乘，相加，相乘还是高斯分布吗？\" class=\"headerlink\" title=\"高斯分布的数乘，相加，相乘还是高斯分布吗？\"></a>高斯分布的数乘，相加，相乘还是高斯分布吗？</h1><p>&emsp;首先当随机变量$Y=g(X)$时,两者的PDF具有这样的关系<script type=\"math/tex\">f_{Y}(y)=f_{X}(x)\\lvert\\frac{dx}{dy}\\rvert</script><br>具体来说<script type=\"math/tex\">F_{Y}(y)=P(Y\\leq y)=P\\left(g(X)\\leq y\\right)=P\\left(X\\leq g^{-1}(y)\\right)=F_{X}( g^{-1}(y)) = F_{X}(x)</script><br>知道上述关系之后，考虑随机变量$Y=aX$，其中$X\\sim\\mathcal{N}(\\mu,\\sigma^2)$，则</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}f_{Y}(y)&=\\frac{1}{a}f_{X}(x)\\\\\n&=\\frac{1}{a}f_{X}(\\frac{y}{a})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}a\\sigma}\\exp(-\\frac{(\\frac{y}{a}-\\mu)^2}{2\\sigma^2})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}(a\\sigma)}\\exp(-\\frac{(y-a\\mu)^2}{2(a\\sigma)^2})\n\\end{aligned}</script><p>所以$Y\\sim\\mathcal{N}(a\\mu,(a\\sigma^2))$，可得知高斯分布的数乘还是高斯分布。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"高斯分布的数乘，相加，相乘还是高斯分布吗？\"><a href=\"#高斯分布的数乘，相加，相乘还是高斯分布吗？\" class=\"headerlink\" title=\"高斯分布的数乘，相加，相乘还是高斯分布吗？\"></a>高斯分布的数乘，相加，相乘还是高斯分布吗？</h1><p>&emsp;首先当随机变量$Y=g(X)$时,两者的PDF具有这样的关系<script type=\"math/tex\">f_{Y}(y)=f_{X}(x)\\lvert\\frac{dx}{dy}\\rvert</script><br>具体来说<script type=\"math/tex\">F_{Y}(y)=P(Y\\leq y)=P\\left(g(X)\\leq y\\right)=P\\left(X\\leq g^{-1}(y)\\right)=F_{X}( g^{-1}(y)) = F_{X}(x)</script><br>知道上述关系之后，考虑随机变量$Y=aX$，其中$X\\sim\\mathcal{N}(\\mu,\\sigma^2)$，则</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}f_{Y}(y)&=\\frac{1}{a}f_{X}(x)\\\\\n&=\\frac{1}{a}f_{X}(\\frac{y}{a})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}a\\sigma}\\exp(-\\frac{(\\frac{y}{a}-\\mu)^2}{2\\sigma^2})\\\\\n&=\\frac{1}{\\sqrt{2\\pi}(a\\sigma)}\\exp(-\\frac{(y-a\\mu)^2}{2(a\\sigma)^2})\n\\end{aligned}</script><p>所以$Y\\sim\\mathcal{N}(a\\mu,(a\\sigma^2))$，可得知高斯分布的数乘还是高斯分布。</p>\n"},{"title":"论文导读：使用半定规划的方法进行数据降维--通过最大最小距离","date":"2022-06-01T13:39:59.000Z","thumbnail":"https://tva1.sinaimg.cn/large/94aee95bgy1h2t3yd6b20j21z4140166.jpg","mathjax":true,"_content":"( 论文导读：使用半定规划的方法进行数据降维--通过最大最小距离[Max-min distance analysis by using sequential SDP relaxation for dimension reduction])\n*可能是全网第一篇讲解此篇论文的博客*\n\n本文旨在记录阅读文献时的想法，并不适合小白，有一定数据降维经验的小伙伴会更快上手。\n原文地址：[Max-min distance analysis by using sequential SDP relaxation for dimension reduction](https://ieeexplore.ieee.org/document/5611542)\n本文的目的是实现数据降维，需要首先了解普通的PCA和LDA[Linear discriminate analysis]的算法。\nLDA的算法是通过最大化类与类之间的距离和最小化同类之间的方差为目标的算法。目标函数设定为$$\\text{objective} : min  \\frac{W^TS_{b}W}{W^TS_{w}W}$$\n具体的请移步：[LDA](https://blog.csdn.net/qq_16137569/article/details/82385050?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159842013019724843303257%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=159842013019724843303257&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-82385050.pc_ecpm_v3_pc_rank_v3&utm_term=LDA&spm=1018.2118.3001.4187)\n# 1. 系统模型\n\n&emsp; 本文的方法提出一个新的优化目标：最大化类别间的最小距离。根据木桶原理，分类的性能瓶颈肯定受限于最小的距离的两个类别。最小的距离被优化了，那么整体的性能肯定会得到优化。\n&emsp; 设定好了目标函数以后我们可以显式的把优化问题表达出来：\n&emsp; 考虑一个$C$种类别的分类问题，假设每一个类别之内的数据点的分布都是同样方差(==不同也没有关系==)的高斯分布。大概如下如图所示：\n![同方差的3类分布](https://img-blog.csdnimg.cn/20200826192530326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n对于每一个$\\omega_i$类别都有条件分布$p(\\boldsymbol{x}|\\omega_i)=\\mathcal N(\\mathbf{\\mu_i,\\Sigma|\\omega_i})$,其中$\\boldsymbol{x}\\in\\mathbb{R}^m$,$\\mathbf{y}=\\boldsymbol{W}^T\\boldsymbol{x}\\in\\mathbb{R}^{ {d} }$,其中$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times{d} }$，$\\boldsymbol{x}$是原数据向量，$\\boldsymbol{y}$是降维后的数据向量，$\\boldsymbol{W}$是降维矩阵。==*此处原文章出错*==\n![原文](https://img-blog.csdnimg.cn/20200828095447987.png#pic_center)\n## 1.1数学表示\n&emsp;每个类别间的数据的方差可以随意假设，文章中假设$\\boldsymbol{\\Sigma}=\\boldsymbol{I}$,现在$\\omega_i$和$\\omega_j$两个类别的中心距离变成了\n$$\\begin{aligned}\n\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})&=tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})\\\\subject\\;{to}\\boldsymbol{W}^T\\boldsymbol{W}&=\\boldsymbol{I}_d\n\\end{aligned}$$\n$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$是为了让$\\mathbf{W}$单位化，其中$\\boldsymbol{D}_{ij}$是矩阵:$$\\boldsymbol{D}_{ij}=(\\mu_i-\\mu_j)(\\mu_i-\\mu_j)^T,$$\n这个矩阵被称为类别$\\omega_i$和$\\omega_j$的距离矩阵，这个矩阵的迹就是未变换前的两个类别中心的距离了。变换后的迹既是:$$tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})=\\|\\boldsymbol{W^T(\\mu_i-\\mu_j)}\\|^2.$$\n&emsp;我们的优化目标便是这个了，找到所有类别中心距离最短的距离MMDA(Max-min distance analysis):$$\\underset{\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d}{\\text{max}} \\quad \\underset{1\\leq{i}\\leq{j}\\leq{C}}{\\text{min}}\\quad\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})。$$\n这乍一看，这个优化问题咋做啊，无从下手啊，每次都遍历一边吗，怎么求导啊？别着急，聪明的作者做了一个变形。\n***\n## 1.2优化模型\n&emsp;引入一个辅助变量$t$，令$t={\\text{min}\\quad\\underset{1 \\leq {i}  <  j  \\leq  {c}}\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})}，$这样模型就变成了\n$$\\begin{aligned}\n\\text{max}\\quad& t\\\\\n\\text{s.t.}\\quad&Tr(\\boldsymbol{D}_{ij}\\mathbf{X})\\geq{t},1\\leq{i}<j\\leq{C}.\n\\end{aligned}$$\n诶，这样一看是不是问题有点头绪了，学习过凸优化的同学应该能看出来这好像是半定规划的问题。但是半定规划有一个$\\boldsymbol{X}\\succeq\\mathbf{0}$啊。这里需要嘛？而且这里好像对$\\boldsymbol{X}$的限制没有体现出$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$。\n&emsp;你想得没错，这里需要插入一条引理\n\n 1. >如果$\\Omega_1是这样的 集合：\\Omega_1=\\{\\mathbf{X}|\\mathbf{X}=\\mathbf{W}\\mathbf{W^T},\\mathbf{W^T}\\mathbf{W}=\\boldsymbol{I}_d\\}，而\\Omega_2=\\{\\mathbf{X}|Tr(\\mathbf{X})=d,\\mathbf{0}\\preceq\\mathbf{X}\\preceq{\\mathbf{I}}\\}$。那么$\\Omega_2$是$\\Omega_1$的最小凸包，$\\Omega_1$是$\\Omega_2$的极点\n\n什么？你不知道什么是极点，什么是凸包？自己百度去。好吧，集合的==极点==就是==不能集合中其他的点线性表示出来的点==，==凸包==就是包含此集合的最小凸集。所以引理1的证明非常直观，直接用定义即可。$\\Omega_1$和$\\Omega_2$的本质差在哪？\n$\\Omega_1$规定了$rank(\\boldsymbol{X})=d$,而$\\Omega_2$没有做此规定，所以$\\Omega_2$中的$X$的rank可以是$1\\leq rank(\\boldsymbol{X})\\leq{m}$。\n&emsp;怎么证明引理1呢？我给出一个不严谨的证明\n>$\\Omega_1\\subseteq\\Omega_2$很明显，并且很明显$rank(\\boldsymbol{X})\\not =d$的$X_1$,$X_2\\in\\Omega_2$的线性组合不可能表示成$rank(\\boldsymbol{X}) =d$且$Tr(\\boldsymbol{X})=d$\n*还是给出一个*证明吧，写这玩意太费时了![在这里插入图片描述](https://img-blog.csdnimg.cn/20200922152310626.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n\n好了，此时的可行域变大了，可行域从极点变成了凸包，这就是我们文章中松弛所在的地方。这时松弛的优化问题变成了\n$$\\begin{aligned}\n\\text{min}\\quad&-t\\\\\ns.t\\quad&tr(\\boldsymbol{A}_{ij}\\boldsymbol{X})\\geq\tt\\quad1\\leq{i}<j\\leq{C}\\\\\n&tr{(\\boldsymbol{X})}=d\\\\\n&0\\preceq\\boldsymbol{X}\\preceq\\boldsymbol{I}\n\\end{aligned}\n$$\n这是不是一个标准的SDP(semidefinte programming)问题?\n啥？你不知道啥是SDP？这里简单的说一下SDP的标准形式吧\n>$$\\begin{aligned}\n{minimize} \\quad& \\mathbf {tr}(CX) \\\\\n{subject\\quad to}\\quad& \\mathbf{tr}(A_iX) = b_i, \\quad i=1,\\ldots,p \\\\\n                  & X \\succeq 0,\n\\end{aligned}$$\n\n不会吧不会吧，不会到了这一步还有人不知道怎么把问题变成标准的SDP形式吧。没办法了，我只好写得详细一些\n$$\n\\begin{aligned}\n111\n\\end{aligned}\n$$\n","source":"_posts/论文导读：使用松弛半定规划的方法进行数据降维--通过最大最小距离.md","raw":"---\ntitle: 论文导读：使用半定规划的方法进行数据降维--通过最大最小距离\ndate: 2022-06-01 21:39:59\ntags:\ncategories: 信号\nthumbnail: https://tva1.sinaimg.cn/large/94aee95bgy1h2t3yd6b20j21z4140166.jpg\nmathjax: true\n---\n( 论文导读：使用半定规划的方法进行数据降维--通过最大最小距离[Max-min distance analysis by using sequential SDP relaxation for dimension reduction])\n*可能是全网第一篇讲解此篇论文的博客*\n\n本文旨在记录阅读文献时的想法，并不适合小白，有一定数据降维经验的小伙伴会更快上手。\n原文地址：[Max-min distance analysis by using sequential SDP relaxation for dimension reduction](https://ieeexplore.ieee.org/document/5611542)\n本文的目的是实现数据降维，需要首先了解普通的PCA和LDA[Linear discriminate analysis]的算法。\nLDA的算法是通过最大化类与类之间的距离和最小化同类之间的方差为目标的算法。目标函数设定为$$\\text{objective} : min  \\frac{W^TS_{b}W}{W^TS_{w}W}$$\n具体的请移步：[LDA](https://blog.csdn.net/qq_16137569/article/details/82385050?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159842013019724843303257%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=159842013019724843303257&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-82385050.pc_ecpm_v3_pc_rank_v3&utm_term=LDA&spm=1018.2118.3001.4187)\n# 1. 系统模型\n\n&emsp; 本文的方法提出一个新的优化目标：最大化类别间的最小距离。根据木桶原理，分类的性能瓶颈肯定受限于最小的距离的两个类别。最小的距离被优化了，那么整体的性能肯定会得到优化。\n&emsp; 设定好了目标函数以后我们可以显式的把优化问题表达出来：\n&emsp; 考虑一个$C$种类别的分类问题，假设每一个类别之内的数据点的分布都是同样方差(==不同也没有关系==)的高斯分布。大概如下如图所示：\n![同方差的3类分布](https://img-blog.csdnimg.cn/20200826192530326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n对于每一个$\\omega_i$类别都有条件分布$p(\\boldsymbol{x}|\\omega_i)=\\mathcal N(\\mathbf{\\mu_i,\\Sigma|\\omega_i})$,其中$\\boldsymbol{x}\\in\\mathbb{R}^m$,$\\mathbf{y}=\\boldsymbol{W}^T\\boldsymbol{x}\\in\\mathbb{R}^{ {d} }$,其中$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times{d} }$，$\\boldsymbol{x}$是原数据向量，$\\boldsymbol{y}$是降维后的数据向量，$\\boldsymbol{W}$是降维矩阵。==*此处原文章出错*==\n![原文](https://img-blog.csdnimg.cn/20200828095447987.png#pic_center)\n## 1.1数学表示\n&emsp;每个类别间的数据的方差可以随意假设，文章中假设$\\boldsymbol{\\Sigma}=\\boldsymbol{I}$,现在$\\omega_i$和$\\omega_j$两个类别的中心距离变成了\n$$\\begin{aligned}\n\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})&=tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})\\\\subject\\;{to}\\boldsymbol{W}^T\\boldsymbol{W}&=\\boldsymbol{I}_d\n\\end{aligned}$$\n$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$是为了让$\\mathbf{W}$单位化，其中$\\boldsymbol{D}_{ij}$是矩阵:$$\\boldsymbol{D}_{ij}=(\\mu_i-\\mu_j)(\\mu_i-\\mu_j)^T,$$\n这个矩阵被称为类别$\\omega_i$和$\\omega_j$的距离矩阵，这个矩阵的迹就是未变换前的两个类别中心的距离了。变换后的迹既是:$$tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})=\\|\\boldsymbol{W^T(\\mu_i-\\mu_j)}\\|^2.$$\n&emsp;我们的优化目标便是这个了，找到所有类别中心距离最短的距离MMDA(Max-min distance analysis):$$\\underset{\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d}{\\text{max}} \\quad \\underset{1\\leq{i}\\leq{j}\\leq{C}}{\\text{min}}\\quad\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})。$$\n这乍一看，这个优化问题咋做啊，无从下手啊，每次都遍历一边吗，怎么求导啊？别着急，聪明的作者做了一个变形。\n***\n## 1.2优化模型\n&emsp;引入一个辅助变量$t$，令$t={\\text{min}\\quad\\underset{1 \\leq {i}  <  j  \\leq  {c}}\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})}，$这样模型就变成了\n$$\\begin{aligned}\n\\text{max}\\quad& t\\\\\n\\text{s.t.}\\quad&Tr(\\boldsymbol{D}_{ij}\\mathbf{X})\\geq{t},1\\leq{i}<j\\leq{C}.\n\\end{aligned}$$\n诶，这样一看是不是问题有点头绪了，学习过凸优化的同学应该能看出来这好像是半定规划的问题。但是半定规划有一个$\\boldsymbol{X}\\succeq\\mathbf{0}$啊。这里需要嘛？而且这里好像对$\\boldsymbol{X}$的限制没有体现出$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$。\n&emsp;你想得没错，这里需要插入一条引理\n\n 1. >如果$\\Omega_1是这样的 集合：\\Omega_1=\\{\\mathbf{X}|\\mathbf{X}=\\mathbf{W}\\mathbf{W^T},\\mathbf{W^T}\\mathbf{W}=\\boldsymbol{I}_d\\}，而\\Omega_2=\\{\\mathbf{X}|Tr(\\mathbf{X})=d,\\mathbf{0}\\preceq\\mathbf{X}\\preceq{\\mathbf{I}}\\}$。那么$\\Omega_2$是$\\Omega_1$的最小凸包，$\\Omega_1$是$\\Omega_2$的极点\n\n什么？你不知道什么是极点，什么是凸包？自己百度去。好吧，集合的==极点==就是==不能集合中其他的点线性表示出来的点==，==凸包==就是包含此集合的最小凸集。所以引理1的证明非常直观，直接用定义即可。$\\Omega_1$和$\\Omega_2$的本质差在哪？\n$\\Omega_1$规定了$rank(\\boldsymbol{X})=d$,而$\\Omega_2$没有做此规定，所以$\\Omega_2$中的$X$的rank可以是$1\\leq rank(\\boldsymbol{X})\\leq{m}$。\n&emsp;怎么证明引理1呢？我给出一个不严谨的证明\n>$\\Omega_1\\subseteq\\Omega_2$很明显，并且很明显$rank(\\boldsymbol{X})\\not =d$的$X_1$,$X_2\\in\\Omega_2$的线性组合不可能表示成$rank(\\boldsymbol{X}) =d$且$Tr(\\boldsymbol{X})=d$\n*还是给出一个*证明吧，写这玩意太费时了![在这里插入图片描述](https://img-blog.csdnimg.cn/20200922152310626.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)\n\n好了，此时的可行域变大了，可行域从极点变成了凸包，这就是我们文章中松弛所在的地方。这时松弛的优化问题变成了\n$$\\begin{aligned}\n\\text{min}\\quad&-t\\\\\ns.t\\quad&tr(\\boldsymbol{A}_{ij}\\boldsymbol{X})\\geq\tt\\quad1\\leq{i}<j\\leq{C}\\\\\n&tr{(\\boldsymbol{X})}=d\\\\\n&0\\preceq\\boldsymbol{X}\\preceq\\boldsymbol{I}\n\\end{aligned}\n$$\n这是不是一个标准的SDP(semidefinte programming)问题?\n啥？你不知道啥是SDP？这里简单的说一下SDP的标准形式吧\n>$$\\begin{aligned}\n{minimize} \\quad& \\mathbf {tr}(CX) \\\\\n{subject\\quad to}\\quad& \\mathbf{tr}(A_iX) = b_i, \\quad i=1,\\ldots,p \\\\\n                  & X \\succeq 0,\n\\end{aligned}$$\n\n不会吧不会吧，不会到了这一步还有人不知道怎么把问题变成标准的SDP形式吧。没办法了，我只好写得详细一些\n$$\n\\begin{aligned}\n111\n\\end{aligned}\n$$\n","slug":"论文导读：使用松弛半定规划的方法进行数据降维--通过最大最小距离","published":1,"updated":"2022-06-01T14:23:35.776Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl3wj0ge2000jgsw1fw970zkp","content":"<p>( 论文导读：使用半定规划的方法进行数据降维—通过最大最小距离[Max-min distance analysis by using sequential SDP relaxation for dimension reduction])<br><em>可能是全网第一篇讲解此篇论文的博客</em></p>\n<p>本文旨在记录阅读文献时的想法，并不适合小白，有一定数据降维经验的小伙伴会更快上手。<br>原文地址：<a href=\"https://ieeexplore.ieee.org/document/5611542\">Max-min distance analysis by using sequential SDP relaxation for dimension reduction</a><br>本文的目的是实现数据降维，需要首先了解普通的PCA和LDA[Linear discriminate analysis]的算法。<br>LDA的算法是通过最大化类与类之间的距离和最小化同类之间的方差为目标的算法。目标函数设定为<script type=\"math/tex\">\\text{objective} : min  \\frac{W^TS_{b}W}{W^TS_{w}W}</script><br>具体的请移步：<a href=\"https://blog.csdn.net/qq_16137569/article/details/82385050?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159842013019724843303257%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=159842013019724843303257&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-82385050.pc_ecpm_v3_pc_rank_v3&amp;utm_term=LDA&amp;spm=1018.2118.3001.4187\">LDA</a></p>\n<h1 id=\"1-系统模型\"><a href=\"#1-系统模型\" class=\"headerlink\" title=\"1. 系统模型\"></a>1. 系统模型</h1><p>&emsp; 本文的方法提出一个新的优化目标：最大化类别间的最小距离。根据木桶原理，分类的性能瓶颈肯定受限于最小的距离的两个类别。最小的距离被优化了，那么整体的性能肯定会得到优化。<br>&emsp; 设定好了目标函数以后我们可以显式的把优化问题表达出来：<br>&emsp; 考虑一个$C$种类别的分类问题，假设每一个类别之内的数据点的分布都是同样方差(==不同也没有关系==)的高斯分布。大概如下如图所示：<br><img src=\"https://img-blog.csdnimg.cn/20200826192530326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"同方差的3类分布\"><br>对于每一个$\\omega_i$类别都有条件分布$p(\\boldsymbol{x}|\\omega_i)=\\mathcal N(\\mathbf{\\mu_i,\\Sigma|\\omega_i})$,其中$\\boldsymbol{x}\\in\\mathbb{R}^m$,$\\mathbf{y}=\\boldsymbol{W}^T\\boldsymbol{x}\\in\\mathbb{R}^{ {d} }$,其中$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times{d} }$，$\\boldsymbol{x}$是原数据向量，$\\boldsymbol{y}$是降维后的数据向量，$\\boldsymbol{W}$是降维矩阵。==<em>此处原文章出错</em>==<br><img src=\"https://img-blog.csdnimg.cn/20200828095447987.png#pic_center\" alt=\"原文\"></p>\n<h2 id=\"1-1数学表示\"><a href=\"#1-1数学表示\" class=\"headerlink\" title=\"1.1数学表示\"></a>1.1数学表示</h2><p>&emsp;每个类别间的数据的方差可以随意假设，文章中假设$\\boldsymbol{\\Sigma}=\\boldsymbol{I}$,现在$\\omega_i$和$\\omega_j$两个类别的中心距离变成了</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})&=tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})\\\\subject\\;{to}\\boldsymbol{W}^T\\boldsymbol{W}&=\\boldsymbol{I}_d\n\\end{aligned}</script><p>$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$是为了让$\\mathbf{W}$单位化，其中$\\boldsymbol{D}_{ij}$是矩阵:<script type=\"math/tex\">\\boldsymbol{D}_{ij}=(\\mu_i-\\mu_j)(\\mu_i-\\mu_j)^T,</script><br>这个矩阵被称为类别$\\omega_i$和$\\omega_j$的距离矩阵，这个矩阵的迹就是未变换前的两个类别中心的距离了。变换后的迹既是:<script type=\"math/tex\">tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})=\\|\\boldsymbol{W^T(\\mu_i-\\mu_j)}\\|^2.</script><br>&emsp;我们的优化目标便是这个了，找到所有类别中心距离最短的距离MMDA(Max-min distance analysis):<script type=\"math/tex\">\\underset{\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d}{\\text{max}} \\quad \\underset{1\\leq{i}\\leq{j}\\leq{C}}{\\text{min}}\\quad\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})。</script><br>这乍一看，这个优化问题咋做啊，无从下手啊，每次都遍历一边吗，怎么求导啊？别着急，聪明的作者做了一个变形。</p>\n<hr>\n<h2 id=\"1-2优化模型\"><a href=\"#1-2优化模型\" class=\"headerlink\" title=\"1.2优化模型\"></a>1.2优化模型</h2><p>&emsp;引入一个辅助变量$t$，令$t={\\text{min}\\quad\\underset{1 \\leq {i}  &lt;  j  \\leq  {c}}\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})}，$这样模型就变成了</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\text{max}\\quad& t\\\\\n\\text{s.t.}\\quad&Tr(\\boldsymbol{D}_{ij}\\mathbf{X})\\geq{t},1\\leq{i}<j\\leq{C}.\n\\end{aligned}</script><p>诶，这样一看是不是问题有点头绪了，学习过凸优化的同学应该能看出来这好像是半定规划的问题。但是半定规划有一个$\\boldsymbol{X}\\succeq\\mathbf{0}$啊。这里需要嘛？而且这里好像对$\\boldsymbol{X}$的限制没有体现出$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$。<br>&emsp;你想得没错，这里需要插入一条引理</p>\n<ol>\n<li><blockquote>\n<p>如果$\\Omega_1是这样的 集合：\\Omega_1=\\{\\mathbf{X}|\\mathbf{X}=\\mathbf{W}\\mathbf{W^T},\\mathbf{W^T}\\mathbf{W}=\\boldsymbol{I}_d\\}，而\\Omega_2=\\{\\mathbf{X}|Tr(\\mathbf{X})=d,\\mathbf{0}\\preceq\\mathbf{X}\\preceq{\\mathbf{I}}\\}$。那么$\\Omega_2$是$\\Omega_1$的最小凸包，$\\Omega_1$是$\\Omega_2$的极点</p>\n</blockquote>\n</li>\n</ol>\n<p>什么？你不知道什么是极点，什么是凸包？自己百度去。好吧，集合的==极点==就是==不能集合中其他的点线性表示出来的点==，==凸包==就是包含此集合的最小凸集。所以引理1的证明非常直观，直接用定义即可。$\\Omega_1$和$\\Omega_2$的本质差在哪？<br>$\\Omega_1$规定了$rank(\\boldsymbol{X})=d$,而$\\Omega_2$没有做此规定，所以$\\Omega_2$中的$X$的rank可以是$1\\leq rank(\\boldsymbol{X})\\leq{m}$。<br>&emsp;怎么证明引理1呢？我给出一个不严谨的证明</p>\n<blockquote>\n<p>$\\Omega_1\\subseteq\\Omega_2$很明显，并且很明显$rank(\\boldsymbol{X})\\not =d$的$X_1$,$X_2\\in\\Omega_2$的线性组合不可能表示成$rank(\\boldsymbol{X}) =d$且$Tr(\\boldsymbol{X})=d$<br><em>还是给出一个</em>证明吧，写这玩意太费时了<img src=\"https://img-blog.csdnimg.cn/20200922152310626.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</blockquote>\n<p>好了，此时的可行域变大了，可行域从极点变成了凸包，这就是我们文章中松弛所在的地方。这时松弛的优化问题变成了</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\text{min}\\quad&-t\\\\\ns.t\\quad&tr(\\boldsymbol{A}_{ij}\\boldsymbol{X})\\geq    t\\quad1\\leq{i}<j\\leq{C}\\\\\n&tr{(\\boldsymbol{X})}=d\\\\\n&0\\preceq\\boldsymbol{X}\\preceq\\boldsymbol{I}\n\\end{aligned}</script><p>这是不是一个标准的SDP(semidefinte programming)问题?<br>啥？你不知道啥是SDP？这里简单的说一下SDP的标准形式吧</p>\n<blockquote>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n{minimize} \\quad& \\mathbf {tr}(CX) \\\\\n{subject\\quad to}\\quad& \\mathbf{tr}(A_iX) = b_i, \\quad i=1,\\ldots,p \\\\\n                  & X \\succeq 0,\n\\end{aligned}</script></blockquote>\n<p>不会吧不会吧，不会到了这一步还有人不知道怎么把问题变成标准的SDP形式吧。没办法了，我只好写得详细一些</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n111\n\\end{aligned}</script>","site":{"data":{}},"excerpt":"","more":"<p>( 论文导读：使用半定规划的方法进行数据降维—通过最大最小距离[Max-min distance analysis by using sequential SDP relaxation for dimension reduction])<br><em>可能是全网第一篇讲解此篇论文的博客</em></p>\n<p>本文旨在记录阅读文献时的想法，并不适合小白，有一定数据降维经验的小伙伴会更快上手。<br>原文地址：<a href=\"https://ieeexplore.ieee.org/document/5611542\">Max-min distance analysis by using sequential SDP relaxation for dimension reduction</a><br>本文的目的是实现数据降维，需要首先了解普通的PCA和LDA[Linear discriminate analysis]的算法。<br>LDA的算法是通过最大化类与类之间的距离和最小化同类之间的方差为目标的算法。目标函数设定为<script type=\"math/tex\">\\text{objective} : min  \\frac{W^TS_{b}W}{W^TS_{w}W}</script><br>具体的请移步：<a href=\"https://blog.csdn.net/qq_16137569/article/details/82385050?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159842013019724843303257%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=159842013019724843303257&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-82385050.pc_ecpm_v3_pc_rank_v3&amp;utm_term=LDA&amp;spm=1018.2118.3001.4187\">LDA</a></p>\n<h1 id=\"1-系统模型\"><a href=\"#1-系统模型\" class=\"headerlink\" title=\"1. 系统模型\"></a>1. 系统模型</h1><p>&emsp; 本文的方法提出一个新的优化目标：最大化类别间的最小距离。根据木桶原理，分类的性能瓶颈肯定受限于最小的距离的两个类别。最小的距离被优化了，那么整体的性能肯定会得到优化。<br>&emsp; 设定好了目标函数以后我们可以显式的把优化问题表达出来：<br>&emsp; 考虑一个$C$种类别的分类问题，假设每一个类别之内的数据点的分布都是同样方差(==不同也没有关系==)的高斯分布。大概如下如图所示：<br><img src=\"https://img-blog.csdnimg.cn/20200826192530326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"同方差的3类分布\"><br>对于每一个$\\omega_i$类别都有条件分布$p(\\boldsymbol{x}|\\omega_i)=\\mathcal N(\\mathbf{\\mu_i,\\Sigma|\\omega_i})$,其中$\\boldsymbol{x}\\in\\mathbb{R}^m$,$\\mathbf{y}=\\boldsymbol{W}^T\\boldsymbol{x}\\in\\mathbb{R}^{ {d} }$,其中$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times{d} }$，$\\boldsymbol{x}$是原数据向量，$\\boldsymbol{y}$是降维后的数据向量，$\\boldsymbol{W}$是降维矩阵。==<em>此处原文章出错</em>==<br><img src=\"https://img-blog.csdnimg.cn/20200828095447987.png#pic_center\" alt=\"原文\"></p>\n<h2 id=\"1-1数学表示\"><a href=\"#1-1数学表示\" class=\"headerlink\" title=\"1.1数学表示\"></a>1.1数学表示</h2><p>&emsp;每个类别间的数据的方差可以随意假设，文章中假设$\\boldsymbol{\\Sigma}=\\boldsymbol{I}$,现在$\\omega_i$和$\\omega_j$两个类别的中心距离变成了</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})&=tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})\\\\subject\\;{to}\\boldsymbol{W}^T\\boldsymbol{W}&=\\boldsymbol{I}_d\n\\end{aligned}</script><p>$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$是为了让$\\mathbf{W}$单位化，其中$\\boldsymbol{D}_{ij}$是矩阵:<script type=\"math/tex\">\\boldsymbol{D}_{ij}=(\\mu_i-\\mu_j)(\\mu_i-\\mu_j)^T,</script><br>这个矩阵被称为类别$\\omega_i$和$\\omega_j$的距离矩阵，这个矩阵的迹就是未变换前的两个类别中心的距离了。变换后的迹既是:<script type=\"math/tex\">tr(\\boldsymbol{W}^T\\boldsymbol{D}_{ij}\\boldsymbol{W})=\\|\\boldsymbol{W^T(\\mu_i-\\mu_j)}\\|^2.</script><br>&emsp;我们的优化目标便是这个了，找到所有类别中心距离最短的距离MMDA(Max-min distance analysis):<script type=\"math/tex\">\\underset{\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d}{\\text{max}} \\quad \\underset{1\\leq{i}\\leq{j}\\leq{C}}{\\text{min}}\\quad\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})。</script><br>这乍一看，这个优化问题咋做啊，无从下手啊，每次都遍历一边吗，怎么求导啊？别着急，聪明的作者做了一个变形。</p>\n<hr>\n<h2 id=\"1-2优化模型\"><a href=\"#1-2优化模型\" class=\"headerlink\" title=\"1.2优化模型\"></a>1.2优化模型</h2><p>&emsp;引入一个辅助变量$t$，令$t={\\text{min}\\quad\\underset{1 \\leq {i}  &lt;  j  \\leq  {c}}\\Delta(\\omega_i,\\omega_j|\\boldsymbol{W})}，$这样模型就变成了</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\text{max}\\quad& t\\\\\n\\text{s.t.}\\quad&Tr(\\boldsymbol{D}_{ij}\\mathbf{X})\\geq{t},1\\leq{i}<j\\leq{C}.\n\\end{aligned}</script><p>诶，这样一看是不是问题有点头绪了，学习过凸优化的同学应该能看出来这好像是半定规划的问题。但是半定规划有一个$\\boldsymbol{X}\\succeq\\mathbf{0}$啊。这里需要嘛？而且这里好像对$\\boldsymbol{X}$的限制没有体现出$\\boldsymbol{W}^T\\boldsymbol{W}=\\boldsymbol{I}_d$。<br>&emsp;你想得没错，这里需要插入一条引理</p>\n<ol>\n<li><blockquote>\n<p>如果$\\Omega_1是这样的 集合：\\Omega_1=\\{\\mathbf{X}|\\mathbf{X}=\\mathbf{W}\\mathbf{W^T},\\mathbf{W^T}\\mathbf{W}=\\boldsymbol{I}_d\\}，而\\Omega_2=\\{\\mathbf{X}|Tr(\\mathbf{X})=d,\\mathbf{0}\\preceq\\mathbf{X}\\preceq{\\mathbf{I}}\\}$。那么$\\Omega_2$是$\\Omega_1$的最小凸包，$\\Omega_1$是$\\Omega_2$的极点</p>\n</blockquote>\n</li>\n</ol>\n<p>什么？你不知道什么是极点，什么是凸包？自己百度去。好吧，集合的==极点==就是==不能集合中其他的点线性表示出来的点==，==凸包==就是包含此集合的最小凸集。所以引理1的证明非常直观，直接用定义即可。$\\Omega_1$和$\\Omega_2$的本质差在哪？<br>$\\Omega_1$规定了$rank(\\boldsymbol{X})=d$,而$\\Omega_2$没有做此规定，所以$\\Omega_2$中的$X$的rank可以是$1\\leq rank(\\boldsymbol{X})\\leq{m}$。<br>&emsp;怎么证明引理1呢？我给出一个不严谨的证明</p>\n<blockquote>\n<p>$\\Omega_1\\subseteq\\Omega_2$很明显，并且很明显$rank(\\boldsymbol{X})\\not =d$的$X_1$,$X_2\\in\\Omega_2$的线性组合不可能表示成$rank(\\boldsymbol{X}) =d$且$Tr(\\boldsymbol{X})=d$<br><em>还是给出一个</em>证明吧，写这玩意太费时了<img src=\"https://img-blog.csdnimg.cn/20200922152310626.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n</blockquote>\n<p>好了，此时的可行域变大了，可行域从极点变成了凸包，这就是我们文章中松弛所在的地方。这时松弛的优化问题变成了</p>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\text{min}\\quad&-t\\\\\ns.t\\quad&tr(\\boldsymbol{A}_{ij}\\boldsymbol{X})\\geq    t\\quad1\\leq{i}<j\\leq{C}\\\\\n&tr{(\\boldsymbol{X})}=d\\\\\n&0\\preceq\\boldsymbol{X}\\preceq\\boldsymbol{I}\n\\end{aligned}</script><p>这是不是一个标准的SDP(semidefinte programming)问题?<br>啥？你不知道啥是SDP？这里简单的说一下SDP的标准形式吧</p>\n<blockquote>\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n{minimize} \\quad& \\mathbf {tr}(CX) \\\\\n{subject\\quad to}\\quad& \\mathbf{tr}(A_iX) = b_i, \\quad i=1,\\ldots,p \\\\\n                  & X \\succeq 0,\n\\end{aligned}</script></blockquote>\n<p>不会吧不会吧，不会到了这一步还有人不知道怎么把问题变成标准的SDP形式吧。没办法了，我只好写得详细一些</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n111\n\\end{aligned}</script>"}],"PostAsset":[],"PostCategory":[{"post_id":"cl3wj0gdu0005gsw1a0fo1stj","category_id":"cl3wj0gdr0002gsw1a3zm4drc","_id":"cl3wj0gdz000dgsw1gn903whr"},{"post_id":"cl3wj0gdf0000gsw107vl0y5e","category_id":"cl3wj0gdr0002gsw1a3zm4drc","_id":"cl3wj0ge2000hgsw15xr7dczb"},{"post_id":"cl3wj0gdx000agsw1flmdbc0l","category_id":"cl3wj0gdw0007gsw16jqsegb3","_id":"cl3wj0ge4000kgsw1hfrb7of9"},{"post_id":"cl3wj0gdp0001gsw12ndf4s4v","category_id":"cl3wj0gdw0007gsw16jqsegb3","_id":"cl3wj0ge5000mgsw1atj155au"},{"post_id":"cl3wj0gdy000cgsw19g4c8q2e","category_id":"cl3wj0gdr0002gsw1a3zm4drc","_id":"cl3wj0ge5000ogsw1hflpf90b"},{"post_id":"cl3wj0ge1000ggsw12tvo6ar1","category_id":"cl3wj0gdr0002gsw1a3zm4drc","_id":"cl3wj0ge5000pgsw1b6h6cgf8"},{"post_id":"cl3wj0gdt0004gsw18txz4v6t","category_id":"cl3wj0gdr0002gsw1a3zm4drc","_id":"cl3wj0ge5000rgsw199dgb7ws"},{"post_id":"cl3wj0ge2000jgsw1fw970zkp","category_id":"cl3wj0gdr0002gsw1a3zm4drc","_id":"cl3wj0ge5000sgsw1bynh28yj"}],"PostTag":[{"post_id":"cl3wj0gdu0005gsw1a0fo1stj","tag_id":"cl3wj0gds0003gsw1brb66hh3","_id":"cl3wj0gdx0009gsw1aup1g393"},{"post_id":"cl3wj0gdf0000gsw107vl0y5e","tag_id":"cl3wj0gds0003gsw1brb66hh3","_id":"cl3wj0gdy000bgsw1218e7are"},{"post_id":"cl3wj0gdp0001gsw12ndf4s4v","tag_id":"cl3wj0gdw0008gsw13rcd5zmr","_id":"cl3wj0ge2000igsw139uu1u60"},{"post_id":"cl3wj0gdt0004gsw18txz4v6t","tag_id":"cl3wj0gds0003gsw1brb66hh3","_id":"cl3wj0ge5000ngsw18h7l9f2g"},{"post_id":"cl3wj0gdx000agsw1flmdbc0l","tag_id":"cl3wj0ge4000lgsw1cldlb9gt","_id":"cl3wj0ge5000qgsw1fdf2ec95"}],"Tag":[{"name":"统计学","_id":"cl3wj0gds0003gsw1brb66hh3"},{"name":"tensorflow","_id":"cl3wj0gdw0008gsw13rcd5zmr"},{"name":"Pytorch","_id":"cl3wj0ge4000lgsw1cldlb9gt"}]}}