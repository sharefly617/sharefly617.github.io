---
title: 论文导读：使用半定规划的方法进行数据降维--通过最大最小距离
date: 2022-06-01 21:39:59
tags:
categories: 信号
thumbnail: https://tva1.sinaimg.cn/large/94aee95bgy1h2t3yd6b20j21z4140166.jpg
mathjax: true
---
<meta name="referrer" content="no-referrer" />
( 论文导读：使用半定规划的方法进行数据降维--通过最大最小距离[Max-min distance analysis by using sequential SDP relaxation for dimension reduction])
*可能是全网第一篇讲解此篇论文的博客*

本文旨在记录阅读文献时的想法，并不适合小白，有一定数据降维经验的小伙伴会更快上手。
原文地址：[Max-min distance analysis by using sequential SDP relaxation for dimension reduction](https://ieeexplore.ieee.org/document/5611542)
本文的目的是实现数据降维，需要首先了解普通的PCA和LDA[Linear discriminate analysis]的算法。
LDA的算法是通过最大化类与类之间的距离和最小化同类之间的方差为目标的算法。目标函数设定为$$\text{objective} : min  \frac{W^TS_{b}W}{W^TS_{w}W}$$
具体的请移步：[LDA](https://blog.csdn.net/qq_16137569/article/details/82385050?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159842013019724843303257%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=159842013019724843303257&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-82385050.pc_ecpm_v3_pc_rank_v3&utm_term=LDA&spm=1018.2118.3001.4187)
# 1. 系统模型

&emsp; 本文的方法提出一个新的优化目标：最大化类别间的最小距离。根据木桶原理，分类的性能瓶颈肯定受限于最小的距离的两个类别。最小的距离被优化了，那么整体的性能肯定会得到优化。
&emsp; 设定好了目标函数以后我们可以显式的把优化问题表达出来：
&emsp; 考虑一个$C$种类别的分类问题，假设每一个类别之内的数据点的分布都是同样方差(==不同也没有关系==)的高斯分布。大概如下如图所示：
![同方差的3类分布](https://img-blog.csdnimg.cn/20200826192530326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)
对于每一个$\omega_i$类别都有条件分布$p(\boldsymbol{x}|\omega_i)=\mathcal N(\mathbf{\mu_i,\Sigma|\omega_i})$,其中$\boldsymbol{x}\in\mathbb{R}^m$,$\mathbf{y}=\boldsymbol{W}^T\boldsymbol{x}\in\mathbb{R}^{ {d} }$,其中$\boldsymbol{W}\in\mathbb{R}^{m\times{d} }$，$\boldsymbol{x}$是原数据向量，$\boldsymbol{y}$是降维后的数据向量，$\boldsymbol{W}$是降维矩阵。==*此处原文章出错*==
![原文](https://img-blog.csdnimg.cn/20200828095447987.png#pic_center)
## 1.1数学表示
&emsp;每个类别间的数据的方差可以随意假设，文章中假设$\boldsymbol{\Sigma}=\boldsymbol{I}$,现在$\omega_i$和$\omega_j$两个类别的中心距离变成了
$$\begin{aligned}
\Delta(\omega_i,\omega_j|\boldsymbol{W})&=tr(\boldsymbol{W}^T\boldsymbol{D}_{ij}\boldsymbol{W})\\subject\;{to}\boldsymbol{W}^T\boldsymbol{W}&=\boldsymbol{I}_d
\end{aligned}$$
$\boldsymbol{W}^T\boldsymbol{W}=\boldsymbol{I}_d$是为了让$\mathbf{W}$单位化，其中$\boldsymbol{D}_{ij}$是矩阵:$$\boldsymbol{D}_{ij}=(\mu_i-\mu_j)(\mu_i-\mu_j)^T,$$
这个矩阵被称为类别$\omega_i$和$\omega_j$的距离矩阵，这个矩阵的迹就是未变换前的两个类别中心的距离了。变换后的迹既是:$$tr(\boldsymbol{W}^T\boldsymbol{D}_{ij}\boldsymbol{W})=\|\boldsymbol{W^T(\mu_i-\mu_j)}\|^2.$$
&emsp;我们的优化目标便是这个了，找到所有类别中心距离最短的距离MMDA(Max-min distance analysis):$$\underset{\boldsymbol{W}^T\boldsymbol{W}=\boldsymbol{I}_d}{\text{max}} \quad \underset{1\leq{i}\leq{j}\leq{C}}{\text{min}}\quad\Delta(\omega_i,\omega_j|\boldsymbol{W})。$$
这乍一看，这个优化问题咋做啊，无从下手啊，每次都遍历一边吗，怎么求导啊？别着急，聪明的作者做了一个变形。
***
## 1.2优化模型
&emsp;引入一个辅助变量$t$，令$t={\text{min}\quad\underset{1 \leq {i}  <  j  \leq  {c}}\Delta(\omega_i,\omega_j|\boldsymbol{W})}，$这样模型就变成了
$$\begin{aligned}
\text{max}\quad& t\\
\text{s.t.}\quad&Tr(\boldsymbol{D}_{ij}\mathbf{X})\geq{t},1\leq{i}<j\leq{C}.
\end{aligned}$$
诶，这样一看是不是问题有点头绪了，学习过凸优化的同学应该能看出来这好像是半定规划的问题。但是半定规划有一个$\boldsymbol{X}\succeq\mathbf{0}$啊。这里需要嘛？而且这里好像对$\boldsymbol{X}$的限制没有体现出$\boldsymbol{W}^T\boldsymbol{W}=\boldsymbol{I}_d$。
&emsp;你想得没错，这里需要插入一条引理

 1. >如果$\Omega_1是这样的 集合：\Omega_1=\{\mathbf{X}|\mathbf{X}=\mathbf{W}\mathbf{W^T},\mathbf{W^T}\mathbf{W}=\boldsymbol{I}_d\}，而\Omega_2=\{\mathbf{X}|Tr(\mathbf{X})=d,\mathbf{0}\preceq\mathbf{X}\preceq{\mathbf{I}}\}$。那么$\Omega_2$是$\Omega_1$的最小凸包，$\Omega_1$是$\Omega_2$的极点

什么？你不知道什么是极点，什么是凸包？自己百度去。好吧，集合的==极点==就是==不能集合中其他的点线性表示出来的点==，==凸包==就是包含此集合的最小凸集。所以引理1的证明非常直观，直接用定义即可。$\Omega_1$和$\Omega_2$的本质差在哪？
$\Omega_1$规定了$rank(\boldsymbol{X})=d$,而$\Omega_2$没有做此规定，所以$\Omega_2$中的$X$的rank可以是$1\leq rank(\boldsymbol{X})\leq{m}$。
&emsp;怎么证明引理1呢？我给出一个不严谨的证明
>$\Omega_1\subseteq\Omega_2$很明显，并且很明显$rank(\boldsymbol{X})\not =d$的$X_1$,$X_2\in\Omega_2$的线性组合不可能表示成$rank(\boldsymbol{X}) =d$且$Tr(\boldsymbol{X})=d$
*还是给出一个*证明吧，写这玩意太费时了![在这里插入图片描述](https://img-blog.csdnimg.cn/20200922152310626.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJlNzI3MTg2NjMw,size_16,color_FFFFFF,t_70#pic_center)

好了，此时的可行域变大了，可行域从极点变成了凸包，这就是我们文章中松弛所在的地方。这时松弛的优化问题变成了
$$\begin{aligned}
\text{min}\quad&-t\\
s.t\quad&tr(\boldsymbol{A}_{ij}\boldsymbol{X})\geq	t\quad1\leq{i}<j\leq{C}\\
&tr{(\boldsymbol{X})}=d\\
&0\preceq\boldsymbol{X}\preceq\boldsymbol{I}
\end{aligned}
$$
这是不是一个标准的SDP(semidefinte programming)问题?
啥？你不知道啥是SDP？这里简单的说一下SDP的标准形式吧
>$$\begin{aligned}
{minimize} \quad& \mathbf {tr}(CX) \\
{subject\quad to}\quad& \mathbf{tr}(A_iX) = b_i, \quad i=1,\ldots,p \\
                  & X \succeq 0,
\end{aligned}$$

不会吧不会吧，不会到了这一步还有人不知道怎么把问题变成标准的SDP形式吧。没办法了，我只好写得详细一些
$$
\begin{aligned}
111
\end{aligned}
$$
